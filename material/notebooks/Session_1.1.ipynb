{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import torch as tr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split   \n",
    "from collections import Counter\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "from bisect import bisect_left\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "path_glove = './'\n",
    "\n",
    "GLOVE_EMBEDDINGS = True\n",
    "REVIEWS = False\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###load data\n",
    "if REVIEWS:\n",
    "    path = \"./data/\"\n",
    "\n",
    "    filepath_dict = {'yelp': './data/yelp_labelled.txt' ,\n",
    "                     'amazon': './data/amazon_cells_labelled.txt',\n",
    "                     'imdb': './data/imdb_labelled.txt'}\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for source, filepath in filepath_dict.items():\n",
    "        df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
    "        # Add another column filled with the source name\n",
    "        df['source'] = source \n",
    "        df_list.append(df)\n",
    "\n",
    "    df = pd.concat(df_list)\n",
    "\n",
    "    ### prepare data for training\n",
    "    df_yelp = df[df['source'] == 'yelp']\n",
    "\n",
    "    sentences = df_yelp['sentence'].values\n",
    "    y = df_yelp['label'].values\n",
    "\n",
    "    df_yelp['spacified'] = list(nlp.pipe(df_yelp['sentence']))\n",
    "    df_yelp['lemmatized'] = df_yelp.spacified.apply(lambda doc: [t.lemma_ for t in doc])\n",
    "    cnts = Counter(l for doc in df_yelp.lemmatized for l in doc)\n",
    "    vocab = sorted([el[0] for el in cnts.items() if el[1] >= 10])\n",
    "    vocab.insert(0,\" \")\n",
    "    \n",
    "    max_it = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c79eeeaec664f9e831f18db3294d3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=115864.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not REVIEWS:\n",
    "    aggression_annotated_comments = pd.read_csv(\"data/aggression_annotated_comments.tsv\", sep=\"\\t\")\n",
    "    agression_annotation = pd.read_csv(\"data/agression_annotation.tsv\", sep=\"\\t\")\n",
    "\n",
    "    agression_data = pd.merge(aggression_annotated_comments, agression_annotation, on=\"rev_id\")\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for rev_id, rev in tqdm(agression_data.groupby(\"rev_id\")):\n",
    "        sentences.append(rev.iloc[0].comment)\n",
    "        labels.append(rev.aggression.sum()/len(rev) >.5)\n",
    "        \n",
    "    with open(\"data/vocab.txt\") as fin:\n",
    "        vocab,_ = zip(*map(lambda x: x.split(\" \"), fin))\n",
    "        vocab = sorted(list(vocab) + [\"<unk>\"])  \n",
    "        \n",
    "    max_it = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create embedding matrix\n",
    "def index(a, x):\n",
    "    'Locate the leftmost value exactly equal to x'\n",
    "    i = bisect_left(a, x)\n",
    "    if i != len(a) and a[i] == x:\n",
    "        return i\n",
    "    raise ValueError\n",
    "    \n",
    "def create_embedding_matrix(filepath, vocab, embedding_dim):\n",
    "    vocab_size = len(vocab)  \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = tr.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in vocab:\n",
    "#                 idx = word_index[word] \n",
    "                embedding_matrix[index(vocab, word)] = tr.from_numpy(np.array(\n",
    "                                        vector, dtype=np.float32))\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b0cf58444f4c9b9c5d7b506e0139fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "\n",
    "def tokenize_data(comments, vocab, max_sentences=1000):\n",
    "    \n",
    "    max_sentences = np.min([1000, len(comments)])\n",
    "    if not REVIEWS:\n",
    "        y = np.array(labels[:max_sentences])\n",
    "    word_seq = np.empty(max_sentences,dtype=object)\n",
    "\n",
    "    for idx, sen in tqdm(enumerate(comments[:max_sentences])):\n",
    "    #     doc = nlp(str(sen))\n",
    "        word_seq[idx] = []\n",
    "        for token in tokenizer(str(sen)):\n",
    "            if(token.lemma_.lower() in vocab):\n",
    "                word_seq[idx].append(index(vocab,token.lemma_.lower())) \n",
    "                \n",
    "    return word_seq\n",
    "\n",
    "word_seq = tokenize_data(sentences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GLOVE_EMBEDDINGS:\n",
    "    \n",
    "    embedding_dim = 50\n",
    "    embedding_path = '{}/glove.6B/glove.6B.{}d.txt'.format(path_glove, embedding_dim)\n",
    "    \n",
    "else:    \n",
    "    \n",
    "#     embedding_path = \"data/all_comments_vec.txt\"\n",
    "    embedding_path = \"data/aggressive_comments_vec.txt\"\n",
    "#     embedding_path = \"data/non_aggressive_comments_vec.txt\"\n",
    "    embedding_dim = 32\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(\n",
    "    embedding_path,\n",
    "    vocab,  \n",
    "    embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OWN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()  \n",
    "        filter_sizes = [1,2,3,5]\n",
    "        num_filters = 36\n",
    "        \n",
    "        #Embedding layer\n",
    "        self.embedding_layer = nn.Embedding(embedding_matrix.shape[0], embedding_dim)\n",
    "        self.embedding_layer.weight = nn.Parameter(embedding_matrix)\n",
    "        self.embedding_layer.weight.requires_grad = False\n",
    "        \n",
    "        #Convolution layer\n",
    "        self.convolution_layer = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embedding_dim)) for K in filter_sizes])\n",
    "        Ks = [nn.Conv2d(1, num_filters, (K, embedding_dim)) for K in filter_sizes]\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(len(Ks)*num_filters, 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = x.unsqueeze(1)  \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convolution_layer] \n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
    "        x = tr.cat(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        logit = self.linear(x)\n",
    "        return(logit)\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_padding(word_seq, max_len=1000):\n",
    "\n",
    "    input_data = tr.zeros([word_seq.shape[0],max_len],dtype=tr.int64)\n",
    "    for i in range(word_seq.shape[0]):\n",
    "        input_data[i,:len(word_seq[i])] = tr.Tensor(word_seq[i][:max_len])\n",
    "        \n",
    "    return input_data\n",
    "\n",
    "max_row = 0\n",
    "for irow, row in enumerate(word_seq):\n",
    "    if len(row) > max_row:\n",
    "        max_row = len(row)\n",
    "\n",
    "input_data = data_padding(word_seq, np.min([max_row, 500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a7155ad59eb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m x_train,x_test,y_train,y_test = train_test_split(\n\u001b[0;32m----> 9\u001b[0;31m                                                 \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                                                 test_size=0.2)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# input_data = tr.tensor(input_data)\n",
    "# labels = tr.from_numpy(y[:, np.newaxis])\n",
    "\n",
    "# get the inputs; data is a list of [inputs, labels]\n",
    "x_train,x_test,y_train,y_test = train_test_split(\n",
    "                                                input_data.numpy(), y,  \n",
    "                                                test_size=0.2)\n",
    "\n",
    "x_val,x_test,y_val,y_test = train_test_split(\n",
    "                                                x_test, y_test,  \n",
    "                                                test_size=0.5)\n",
    "\n",
    "\n",
    "loss_hist = []\n",
    "loss_val_hist = []\n",
    "\n",
    "for it in range(max_it):\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(tr.from_numpy(x_train))\n",
    "    loss = criterion(outputs, tr.from_numpy(y_train[:,np.newaxis]).float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_hist.append(loss.detach())\n",
    "    \n",
    "    if it%10==0:\n",
    "        print(\"training loss: \", loss_hist[-1])\n",
    "        \n",
    "        outputs_val = net.forward(tr.from_numpy(x_val))\n",
    "        loss_val_hist.append(criterion(outputs_val, tr.from_numpy(y_val[:,np.newaxis]).float()).detach())\n",
    "        \n",
    "        print(\"validation loss: \",loss_val_hist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_predict = (outputs_val.detach().numpy()>=0).astype(int).ravel()\n",
    "print(classification_report(y_val, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist)\n",
    "plt.plot(np.arange(0,max_it,10),loss_val_hist)\n",
    "plt.legend(['training_loss', 'validation_loss'])\n",
    "plt.savefig('loss.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(input_data):\n",
    "    word_seq = tokenize_data(input_data, vocab)\n",
    "    X = data_padding(word_seq)\n",
    "    output = net.forward(X)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['this was a nice day']\n",
    "output = predict_label(data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "outputs_val = net.forward(tr.from_numpy(x_test))\n",
    "y_predict = (outputs_val.detach().numpy()>=0).astype(int).ravel()\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
