{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Net on classification tasks with different (probably biased) Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/millawell/bias-ml-dh.git#subdirectory=material/notebooks/bias_ml_dh_utils\n",
    "!pip install --upgrade tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import torch as tr\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "from bisect import bisect_left\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "import bias_ml_dh_utils as utils\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can choose between different data sets for the classification task and different Word Embeddings. \n",
    "For starters, we provide the standard glove 50 dimensinal embeddings provided by the authors of the glove paper. If you decide to do Session_1.3, you can load your custom word embeddings here.\n",
    "You can choose between four different classification problems:\n",
    "\n",
    "1. `yelp_sentiment_english`: Sentiment analysis on yelp reviews\n",
    "2. `imdb_sentiment_english`: Sentiment analysis on imdb reviews\n",
    "3. `amazon_sentiment_english`: Sentiment analysis on amazon reviews\n",
    "4. `wikipedia`: Aggressive comments detection on talk pages on wikipedia\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_identifier = \"yelp_sentiment_english\"\n",
    "embedding_path = utils.download_dataset(\"glove.6B.50d\")\n",
    "\n",
    "#data_identifier = \"wikipedia\"\n",
    "#embedding_path = \"data/non_aggressive_comments_vec.txt\"\n",
    "#embedding_path = \"data/aggressive_comments_vec.txt\"\n",
    "#embedding_path = \"data/non_aggressive_comments_vec.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_identifier, vocab):\n",
    "    \n",
    "    sentiment_datasets = [\n",
    "        \"yelp_sentiment_english\",\n",
    "        \"amazon_sentiment_english\",\n",
    "        \"imdb_sentiment_english\",\n",
    "    ]\n",
    "    \n",
    "    if data_identifier in sentiment_datasets:\n",
    "        \n",
    "        path_to_data = utils.download_dataset(data_identifier)\n",
    "\n",
    "        df = pd.read_csv(path_to_data, names=['document', 'label'], sep='\\t')\n",
    "        \n",
    "        labels = df['label'].values\n",
    "        doc_strings = df['document']\n",
    "\n",
    "        \n",
    "    elif data_identifier==\"wikipedia\":\n",
    "        \n",
    "        with open(\"data/wikipedia_toxic_classification_data\", \"rb\") as fin:\n",
    "            doc_strings, labels = pickle.load(fin)\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('data not known')\n",
    "\n",
    "    documents = []\n",
    "    for document in tqdm(tokenizer.pipe(doc_strings),desc=\"tokenize\", total=len(doc_strings)):\n",
    "        new_doc = []\n",
    "        for t in document:\n",
    "            try:\n",
    "                new_doc.append(utils.index_sorted_list(vocab, t.text))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        documents.append(new_doc)\n",
    "        \n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(documents, labels, maxlen, pad_id):\n",
    "    \n",
    "    X = np.zeros((len(documents), max_len), dtype=\"int\") + pad_id\n",
    "\n",
    "    for idoc, doc in tqdm(enumerate(documents), desc=\"pad docs\"):\n",
    "        X[idoc,:len(doc)] = doc[:maxlen]\n",
    "    \n",
    "    X = tr.from_numpy(X)\n",
    "    labels = tr.from_numpy(labels).float()\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        X, labels,  \n",
    "        test_size=0.2\n",
    "    )\n",
    "\n",
    "    x_val, x_test, y_val, y_test = train_test_split(\n",
    "        x_test, y_test,  \n",
    "        test_size=0.5\n",
    "    )\n",
    "    \n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x_train, y_train, batch_size=50):\n",
    "    '''get batch with random samples and equal number of samples from each class.\n",
    "       resulting batch size is at most `batch_size`'''\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for cls in set(y_train.tolist()):\n",
    "        x_batch = x_train[y_train == cls]\n",
    "        y_batch = y_train[y_train == cls]\n",
    "        perm = torch.randperm(x_batch.size(0))\n",
    "        idx = perm[:batch_size//len(set(y_train.tolist()))]\n",
    "        x_batch = x_batch[idx]\n",
    "        y_batch = y_batch[idx]\n",
    "\n",
    "        x_batches.append(x_batch)\n",
    "        y_batches.append(y_batch)\n",
    "\n",
    "    x_batch = torch.cat(x_batches)\n",
    "    y_batch = torch.cat(y_batches)\n",
    "    idx = torch.randperm(x_batch.size(0))\n",
    "    x_batch = x_batch[idx]\n",
    "    y_batch = y_batch[idx]\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "\n",
    "Here, you could change kernel sizes, number of filters, dropout rate etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(Net, self).__init__()  \n",
    "        filter_sizes = [3,4,5]\n",
    "        num_filters = 100\n",
    "        \n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "        #Embedding layer\n",
    "        self.embedding_layer = nn.Embedding(embedding_matrix.shape[0], embedding_dim)\n",
    "        self.embedding_layer.weight = nn.Parameter(tr.from_numpy(embedding_matrix).float())\n",
    "        self.embedding_layer.weight.requires_grad = False\n",
    "        \n",
    "        #Convolution layer\n",
    "        \n",
    "        self.convolution_layer = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embedding_dim)) for K in filter_sizes])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(len(self.convolution_layer)*num_filters, 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = x.unsqueeze(1)  \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convolution_layer] \n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
    "        x = tr.cat(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        logit = self.linear(x)\n",
    "        return(logit.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Here, you could change learning rate (`lr`) and batch size `get_batch(x_train, y_train, batch_size=YOUR_BATCH_SIZE)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(net, x_train, y_train, x_val, y_val, max_it=200):\n",
    "    bestmodel = copy.deepcopy(net)\n",
    "    #sets optimizer and loss function\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.002)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    loss_hist = []\n",
    "    loss_val_hist = []\n",
    "    \n",
    "    for it in tqdm(range(max_it)):\n",
    "        \n",
    "        x_batch, y_batch = get_batch(x_train, y_train)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_hist.append(loss.item())\n",
    "        \n",
    "        every_tenth_iteration = (it%10) == 0\n",
    "        last_iteration = it == (max_it-1)\n",
    "        if every_tenth_iteration or last_iteration:\n",
    "\n",
    "            outputs_val = net.forward(x_val)\n",
    "            val_loss = criterion(outputs_val, y_val)\n",
    "            if len(loss_val_hist) > 0 and val_loss < min(loss_val_hist):\n",
    "                bestmodel = copy.deepcopy(net)\n",
    "            loss_val_hist.append(val_loss.item())\n",
    "\n",
    "            print(\n",
    "                \"training loss: {:0.3f}\".format(loss_hist[-1]),\n",
    "                \"validati loss: {:0.3f}\".format(loss_val_hist[-1]),\n",
    "            )\n",
    "            \n",
    "    y_predict = (outputs_val.detach().numpy()>=0).astype(int).ravel()\n",
    "    print(classification_report(y_val, y_predict))\n",
    "\n",
    "    plt.plot(loss_hist)\n",
    "    plt.plot(np.arange(0,max_it+1,10),loss_val_hist)\n",
    "    plt.legend(['training_loss', 'validation_loss'])\n",
    "    plt.savefig('loss.png', dpi=300)\n",
    "    \n",
    "    outputs_val = net.forward(x_test)\n",
    "    y_predict = (outputs_val.detach().numpy()>=0).astype(int).ravel()\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    return bestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(input_data, max_len, vocab, pad_id, label_names = ['negative','positive']):\n",
    "\n",
    "    \n",
    "    X = torch.zeros((1,max_len)).long() + pad_id\n",
    "    for it, t in enumerate(nlp(input_data)):\n",
    "        try:\n",
    "            X[:,it] = utils.index_sorted_list(vocab, t.text)\n",
    "        except ValueError:\n",
    "            print(f\"`{t.text}` not found in vocab\")\n",
    "\n",
    "    net.eval()\n",
    "    output = net.forward(X)\n",
    "    label = tr.clamp(tr.sign(output.detach()),0,1)\n",
    "\n",
    "    print(\"The predicted label is: \",label_names[int(label)])\n",
    "    print(output)\n",
    "    \n",
    "    return output.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entry points for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding matrix\n",
    "embedding_matrix, vocab = utils.create_embedding_matrix(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "documents, labels = load_data(data_identifier, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at some samples.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sample, vocab):\n",
    "    decoded = []\n",
    "    for token in sample:\n",
    "        decoded.append(vocab[token])\n",
    "    return \" \".join(decoded)\n",
    "\n",
    "random_doc_id = np.random.randint(0,len(documents))\n",
    "print(decode(documents[random_doc_id], vocab), labels[random_doc_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can change the maximum sequence length (`max_len`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "pad_id = utils.index_sorted_list(vocab, \"[PAD]\")\n",
    "\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = prepare_data(documents, labels, max_len, pad_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can change the number of training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(embedding_matrix)\n",
    "\n",
    "num_training_steps = 100\n",
    "net = train_classifier(net, x_train, y_train, x_val, y_val, num_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate resulting model\n",
    "\n",
    "Try out new data! Test your model on biases by classifying sentences that you think include biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = 'this movie was really good'\n",
    "print(data)\n",
    "_ = predict_label(input_sentence,  max_len, vocab, pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the wikipedia corpus, you may use these labels:\n",
    "input_sentence = \"we like you\"\n",
    "_ = predict_label(input_sentence,  max_len, vocab, pad_id, [\"non-aggressive\", \"aggressive\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wh4",
   "language": "python",
   "name": "wh4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
