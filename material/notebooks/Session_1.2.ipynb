{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEAT Word Embedding Association Tests\n",
    "import torch as tr\n",
    "import numpy as np\n",
    "from bisect import bisect_left\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "\n",
    "path_glove = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/vocab.txt\") as fin:\n",
    "    vocab,_ = zip(*map(lambda x: x.split(\" \"), fin))\n",
    "    vocab = sorted(list(vocab) + [\"<unk>\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create embedding matrix\n",
    "def index(a, x):\n",
    "    'Locate the leftmost value exactly equal to x'\n",
    "    i = bisect_left(a, x)\n",
    "    if i != len(a) and a[i] == x:\n",
    "        return i\n",
    "    raise ValueError\n",
    "    \n",
    "def create_embedding_matrix(filepath, vocab, embedding_dim):\n",
    "    vocab_size = len(vocab)  \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = tr.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in vocab:\n",
    "#                 idx = word_index[word] \n",
    "                embedding_matrix[index(vocab, word)] = tr.from_numpy(np.array(\n",
    "                                        vector, dtype=np.float32))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def lookup_embeddings(text, vocab, embedding_matrix):\n",
    "    \n",
    "    embeddings = tr.zeros(len(text), embedding_matrix.shape[1])\n",
    "    \n",
    "    for iword, word in enumerate(text):\n",
    "        for token in tokenizer(str(word)):\n",
    "            if(token.lemma_.lower() in vocab):\n",
    "                embeddings[iword] = embedding_matrix[index(vocab, word)]\n",
    "                \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_path = '{}/glove.6B/glove.6B.{}d.txt'.format(path_glove, embedding_dim)\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(embedding_path, vocab, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ['male', 'man']\n",
    "B = ['female', 'woman']\n",
    "X = ['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career']\n",
    "Y = ['home', 'parents', 'children', 'family', 'cousins', 'marriage']\n",
    "#, 'wedding', 'relatives']\n",
    "\n",
    "\n",
    "\n",
    "def word_attribute_association(w, A, B, vocab):\n",
    "    #s(w,A,B) = mean_a cos(w,a) - mean_b cos(w,b)\n",
    "    A_embed = lookup_embeddings(A, vocab, embedding_matrix)\n",
    "    B_embed = lookup_embeddings(B, vocab, embedding_matrix)\n",
    "    w_embed = lookup_embeddings(w,vocab, embedding_matrix)\n",
    "    \n",
    "    wA = np.dot(w_embed.numpy()/np.linalg.norm(w_embed, axis=1)[:,np.newaxis],\n",
    "                (A_embed.numpy()/np.linalg.norm(A_embed, axis=1)[:,np.newaxis]).T).sum()\n",
    "    wB = np.dot(w_embed.numpy()/np.linalg.norm(w_embed, axis=1)[:,np.newaxis],\n",
    "                (B_embed.numpy()/np.linalg.norm(B_embed, axis=1)[:,np.newaxis]).T).sum()\n",
    "    \n",
    "    return wA/len(A) -  wB/len(B)\n",
    "\n",
    "def test_statistic(A,B,X,Y, vocab):\n",
    "    \n",
    "    wA = 0\n",
    "    \n",
    "    for ix in X:\n",
    "        wA += word_attribute_association([ix], A, B, vocab)\n",
    "        \n",
    "    wB = 0\n",
    "    \n",
    "    for iy in Y:\n",
    "        wB -= word_attribute_association([iy], A, B, vocab)\n",
    "        \n",
    "    return wA+wB\n",
    "\n",
    "def calculate_pvalue(A,B,X,Y, vocab):\n",
    "    \n",
    "    test_stat_orig = test_statistic(A,B,X,Y,vocab)\n",
    "    \n",
    "    union = set(X+Y)\n",
    "    subset_size = len(union)//2\n",
    "    \n",
    "    larger = 0\n",
    "    total = 0\n",
    "    \n",
    "    for subset in tqdm(set(itertools.combinations(union, subset_size))):\n",
    "        total += 1\n",
    "        Xi = list(set(subset))\n",
    "        Yi = list(union - set(subset))\n",
    "        if test_statistic(A, B, Xi, Yi,vocab) > test_stat_orig:\n",
    "            larger += 1\n",
    "    print('num of samples:', total)\n",
    "    return larger/float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1534e59af1ea4100ab77d12b83337971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3432.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = calculate_pvalue(A,B,X,Y, vocab)\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
