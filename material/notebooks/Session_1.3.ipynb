{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'GloVe'...\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 493 (delta 0), reused 2 (delta 0), pack-reused 486\u001b[K\n",
      "Receiving objects: 100% (493/493), 199.00 KiB | 209.00 KiB/s, done.\n",
      "Resolving deltas: 100% (272/272), done.\n",
      "/Users/davidlassner/research/conferences_and_papers/workshops/2020 DHd/website/material/notebooks/GloVe\n",
      "mkdir -p build\n",
      "gcc -c src/vocab_count.c -o build/vocab_count.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "clang: \u001b[0;1;35mwarning: \u001b[0m-lm: 'linker' input unused [-Wunused-command-line-argument]\u001b[0m\n",
      "gcc -c src/cooccur.c -o build/cooccur.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "clang: \u001b[0;1;35mwarning: \u001b[0m-lm: 'linker' input unused [-Wunused-command-line-argument]\u001b[0m\n",
      "gcc -c src/shuffle.c -o build/shuffle.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "clang: \u001b[0;1;35mwarning: \u001b[0m-lm: 'linker' input unused [-Wunused-command-line-argument]\u001b[0m\n",
      "gcc -c src/glove.c -o build/glove.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "clang: \u001b[0;1;35mwarning: \u001b[0m-lm: 'linker' input unused [-Wunused-command-line-argument]\u001b[0m\n",
      "gcc -c src/common.c -o build/common.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "clang: \u001b[0;1;35mwarning: \u001b[0m-lm: 'linker' input unused [-Wunused-command-line-argument]\u001b[0m\n",
      "gcc build/vocab_count.o build/common.o -o build/vocab_count -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc build/cooccur.o build/common.o -o build/cooccur -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc build/shuffle.o build/common.o -o build/shuffle -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc build/glove.o build/common.o -o build/glove -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "/Users/davidlassner/research/conferences_and_papers/workshops/2020 DHd/website/material/notebooks\n"
     ]
    }
   ],
   "source": [
    "!git clone git@github.com:stanfordnlp/GloVe.git\n",
    "%cd gloVe\n",
    "!make\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the aggressive wikipedia comments data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-31 11:43:41--  https://ndownloader.figshare.com/files/7394506\n",
      "Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 34.253.86.128, 54.171.25.236, 108.128.77.47, ...\n",
      "Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|34.253.86.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/7394506/aggression_annotations.tsv [following]\n",
      "--2020-01-31 11:43:41--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/7394506/aggression_annotations.tsv\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.36.218\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.36.218|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30627328 (29M) [binary/octet-stream]\n",
      "Saving to: ‘agression_annotation.tsv’\n",
      "\n",
      "agression_annotatio 100%[===================>]  29.21M  42.8MB/s    in 0.7s    \n",
      "\n",
      "2020-01-31 11:43:42 (42.8 MB/s) - ‘agression_annotation.tsv’ saved [30627328/30627328]\n",
      "\n",
      "--2020-01-31 11:43:42--  https://ndownloader.figshare.com/files/7038038\n",
      "Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 34.242.169.253, 54.171.25.236, 108.128.77.47, ...\n",
      "Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|34.242.169.253|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/7038038/aggression_annotated_comments.tsv [following]\n",
      "--2020-01-31 11:43:42--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/7038038/aggression_annotated_comments.tsv\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.36.218\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.36.218|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 58090178 (55M) [binary/octet-stream]\n",
      "Saving to: ‘aggression_annotated_comments.tsv’\n",
      "\n",
      "aggression_annotate 100%[===================>]  55.40M  49.1MB/s    in 1.1s    \n",
      "\n",
      "2020-01-31 11:43:44 (49.1 MB/s) - ‘aggression_annotated_comments.tsv’ saved [58090178/58090178]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget https://ndownloader.figshare.com/files/7394506 -O agression_annotation.tsv\n",
    "!wget https://ndownloader.figshare.com/files/7038038 -O aggression_annotated_comments.tsv\n",
    "!mv agression_annotation.tsv data/\n",
    "!mv aggression_annotated_comments.tsv data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "aggression_annotated_comments = pd.read_csv(\"data/aggression_annotated_comments.tsv\", sep=\"\\t\")\n",
    "agression_annotation = pd.read_csv(\"data/agression_annotation.tsv\", sep=\"\\t\")\n",
    "\n",
    "agression_data = pd.merge(aggression_annotated_comments, agression_annotation, on=\"rev_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f43b9cfee14afbbcd5a09c27903a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=115864.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "comments = []\n",
    "labels = []\n",
    "for rev_id, rev in tqdm(agression_data.groupby(\"rev_id\")):\n",
    "    comments.append(rev.iloc[0].comment)\n",
    "    labels.append(rev.aggression.sum()/len(rev) >.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split    \n",
    "train_embeddings, X_other, embeddings_labels, y_other  = train_test_split(comments, labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_other, y_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29f2fd8da7b4311b6c86338f0a20603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='storing aggressive comments', max=86898.0, style=Progress…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "\n",
    "with open(\"data/all_comments_plain.txt\", \"w\") as all_fout:\n",
    "    with open(\"data/aggressive_comments_plain.txt\", \"w\") as agg_fout:\n",
    "        with open(\"data/non_aggressive_comments_plain.txt\", \"w\") as non_agg_fout:\n",
    "            for comment, label in tqdm(\n",
    "                zip(tokenizer.pipe(train_embeddings), embeddings_labels),\n",
    "                total=len(train_embeddings),\n",
    "                desc=\"storing comments\"\n",
    "            ):\n",
    "                out_line = \" \".join([t.text.replace(\"\\n\", \"\\t\") for t in comment])\n",
    "                all_fout.write(\"{}\\n\".format(out_line))\n",
    "                if label == 1:\n",
    "                    agg_fout.write(\"{}\\n\".format(out_line))\n",
    "                else:\n",
    "                    non_agg_fout.write(\"{}\\n\".format(out_line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggression_annotated_comments.tsv all_comments_plain.txt\r\n",
      "aggressive_comments_plain.txt     non_aggressive_comments_plain.txt\r\n",
      "agression_annotation.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common.o      cooccur.o     glove.o       shuffle.o     vocab_count.o\r\n",
      "\u001b[31mcooccur\u001b[m\u001b[m       \u001b[31mglove\u001b[m\u001b[m         \u001b[31mshuffle\u001b[m\u001b[m       \u001b[31mvocab_count\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls gloVe/build/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[11G100000 tokens.\u001b[11G200000 tokens.\u001b[11G300000 tokens.\u001b[11G400000 tokens.\u001b[11G500000 tokens.\u001b[11G600000 tokens.\u001b[11G700000 tokens.\u001b[11G800000 tokens.\u001b[11G900000 tokens.\u001b[11G1000000 tokens.\u001b[11G1100000 tokens.\u001b[11G1200000 tokens.\u001b[11G1300000 tokens.\u001b[11G1400000 tokens.\u001b[11G1500000 tokens.\u001b[11G1600000 tokens.\u001b[11G1700000 tokens.\u001b[11G1800000 tokens.\u001b[11G1900000 tokens.\u001b[11G2000000 tokens.\u001b[11G2100000 tokens.\u001b[11G2200000 tokens.\u001b[11G2300000 tokens.\u001b[11G2400000 tokens.\u001b[11G2500000 tokens.\u001b[11G2600000 tokens.\u001b[11G2700000 tokens.\u001b[11G2800000 tokens.\u001b[11G2900000 tokens.\u001b[11G3000000 tokens.\u001b[11G3100000 tokens.\u001b[11G3200000 tokens.\u001b[11G3300000 tokens.\u001b[11G3400000 tokens.\u001b[11G3500000 tokens.\u001b[11G3600000 tokens.\u001b[11G3700000 tokens.\u001b[11G3800000 tokens.\u001b[11G3900000 tokens.\u001b[11G4000000 tokens.\u001b[11G4100000 tokens.\u001b[11G4200000 tokens.\u001b[11G4300000 tokens.\u001b[11G4400000 tokens.\u001b[11G4500000 tokens.\u001b[11G4600000 tokens.\u001b[11G4700000 tokens.\u001b[11G4800000 tokens.\u001b[11G4900000 tokens.\u001b[11G5000000 tokens.\u001b[11G5100000 tokens.\u001b[11G5200000 tokens.\u001b[11G5300000 tokens.\u001b[11G5400000 tokens.\u001b[11G5500000 tokens.\u001b[11G5600000 tokens.\u001b[11G5700000 tokens.\u001b[11G5800000 tokens.\u001b[11G5900000 tokens.\u001b[11G6000000 tokens.\u001b[11G6100000 tokens.\u001b[11G6200000 tokens.\u001b[11G6300000 tokens.\u001b[11G6400000 tokens.\u001b[11G6500000 tokens.\u001b[11G6600000 tokens.\u001b[11G6700000 tokens.\u001b[11G6800000 tokens.\u001b[11G6900000 tokens.\u001b[11G7000000 tokens.\u001b[11G7100000 tokens.\u001b[11G7200000 tokens.\u001b[11G7300000 tokens.\u001b[0GProcessed 7359817 tokens.\n",
      "Counted 224153 unique words.\n",
      "Truncating vocabulary at min count 10.\n",
      "Using vocabulary of size 23261.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gloVe/build/vocab_count -max-vocab 25000 -min-count 10 < data/all_comments_plain.txt > data/vocab.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cooccurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 10485784\n",
      "overflow length: 28521267\n",
      "Reading vocab from file \"data/vocab.txt\"...loaded 23261 words.\n",
      "Building lookup table...table contains 51813977 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[19G900000\u001b[19G1000000\u001b[19G1100000\u001b[19G1200000\u001b[19G1300000\u001b[19G1400000\u001b[19G1500000\u001b[19G1600000\u001b[19G1700000\u001b[19G1800000\u001b[19G1900000\u001b[19G2000000\u001b[19G2100000\u001b[19G2200000\u001b[19G2300000\u001b[19G2400000\u001b[19G2500000\u001b[19G2600000\u001b[19G2700000\u001b[19G2800000\u001b[19G2900000\u001b[19G3000000\u001b[19G3100000\u001b[19G3200000\u001b[19G3300000\u001b[19G3400000\u001b[19G3500000\u001b[19G3600000\u001b[19G3700000\u001b[19G3800000\u001b[19G3900000\u001b[19G4000000\u001b[19G4100000\u001b[19G4200000\u001b[19G4300000\u001b[19G4400000\u001b[19G4500000\u001b[19G4600000\u001b[19G4700000\u001b[19G4800000\u001b[19G4900000\u001b[19G5000000\u001b[19G5100000\u001b[19G5200000\u001b[19G5300000\u001b[19G5400000\u001b[19G5500000\u001b[19G5600000\u001b[19G5700000\u001b[19G5800000\u001b[19G5900000\u001b[19G6000000\u001b[19G6100000\u001b[19G6200000\u001b[19G6300000\u001b[19G6400000\u001b[19G6500000\u001b[0GProcessed 6510651 tokens.\n",
      "Writing cooccurrences to disk........2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[39G2900000 lines.\u001b[39G3000000 lines.\u001b[39G3100000 lines.\u001b[39G3200000 lines.\u001b[39G3300000 lines.\u001b[39G3400000 lines.\u001b[39G3500000 lines.\u001b[39G3600000 lines.\u001b[39G3700000 lines.\u001b[39G3800000 lines.\u001b[39G3900000 lines.\u001b[39G4000000 lines.\u001b[39G4100000 lines.\u001b[39G4200000 lines.\u001b[39G4300000 lines.\u001b[39G4400000 lines.\u001b[39G4500000 lines.\u001b[39G4600000 lines.\u001b[39G4700000 lines.\u001b[39G4800000 lines.\u001b[39G4900000 lines.\u001b[39G5000000 lines.\u001b[39G5100000 lines.\u001b[39G5200000 lines.\u001b[39G5300000 lines.\u001b[39G5400000 lines.\u001b[39G5500000 lines.\u001b[39G5600000 lines.\u001b[39G5700000 lines.\u001b[39G5800000 lines.\u001b[39G5900000 lines.\u001b[39G6000000 lines.\u001b[39G6100000 lines.\u001b[39G6200000 lines.\u001b[39G6300000 lines.\u001b[39G6400000 lines.\u001b[39G6500000 lines.\u001b[39G6600000 lines.\u001b[39G6700000 lines.\u001b[39G6800000 lines.\u001b[39G6900000 lines.\u001b[39G7000000 lines.\u001b[39G7100000 lines.\u001b[39G7200000 lines.\u001b[39G7300000 lines.\u001b[39G7400000 lines.\u001b[39G7500000 lines.\u001b[39G7600000 lines.\u001b[39G7700000 lines.\u001b[39G7800000 lines.\u001b[39G7900000 lines.\u001b[39G8000000 lines.\u001b[39G8100000 lines.\u001b[39G8200000 lines.\u001b[39G8300000 lines.\u001b[39G8400000 lines.\u001b[39G8500000 lines.\u001b[39G8600000 lines.\u001b[39G8700000 lines.\u001b[39G8800000 lines.\u001b[39G8900000 lines.\u001b[39G9000000 lines.\u001b[39G9100000 lines.\u001b[39G9200000 lines.\u001b[39G9300000 lines.\u001b[39G9400000 lines.\u001b[39G9500000 lines.\u001b[39G9600000 lines.\u001b[39G9700000 lines.\u001b[39G9800000 lines.\u001b[39G9900000 lines.\u001b[39G10000000 lines.\u001b[39G10100000 lines.\u001b[39G10200000 lines.\u001b[39G10300000 lines.\u001b[39G10400000 lines.\u001b[39G10500000 lines.\u001b[39G10600000 lines.\u001b[39G10700000 lines.\u001b[39G10800000 lines.\u001b[39G10900000 lines.\u001b[39G11000000 lines.\u001b[39G11100000 lines.\u001b[39G11200000 lines.\u001b[39G11300000 lines.\u001b[39G11400000 lines.\u001b[39G11500000 lines.\u001b[39G11600000 lines.\u001b[39G11700000 lines.\u001b[39G11800000 lines.\u001b[39G11900000 lines.\u001b[39G12000000 lines.\u001b[39G12100000 lines.\u001b[39G12200000 lines.\u001b[39G12300000 lines.\u001b[39G12400000 lines.\u001b[39G12500000 lines.\u001b[39G12600000 lines.\u001b[39G12700000 lines.\u001b[39G12800000 lines.\u001b[39G12900000 lines.\u001b[39G13000000 lines.\u001b[39G13100000 lines.\u001b[39G13200000 lines.\u001b[39G13300000 lines.\u001b[39G13400000 lines.\u001b[39G13500000 lines.\u001b[39G13600000 lines.\u001b[39G13700000 lines.\u001b[39G13800000 lines.\u001b[39G13900000 lines.\u001b[39G14000000 lines.\u001b[39G14100000 lines.\u001b[39G14200000 lines.\u001b[39G14300000 lines.\u001b[39G14400000 lines.\u001b[39G14500000 lines.\u001b[39G14600000 lines.\u001b[39G14700000 lines.\u001b[39G14800000 lines.\u001b[39G14900000 lines.\u001b[39G15000000 lines.\u001b[39G15100000 lines.\u001b[39G15200000 lines.\u001b[39G15300000 lines.\u001b[39G15400000 lines.\u001b[39G15500000 lines.\u001b[39G15600000 lines.\u001b[39G15700000 lines.\u001b[39G15800000 lines.\u001b[0GMerging cooccurrence files: processed 15858731 lines.\n",
      "\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 10485784\n",
      "overflow length: 28521267\n",
      "Reading vocab from file \"data/vocab.txt\"...loaded 23261 words.\n",
      "Building lookup table...table contains 51813977 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[0GProcessed 849166 tokens.\n",
      "Writing cooccurrences to disk........2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[39G2900000 lines.\u001b[0GMerging cooccurrence files: processed 2927335 lines.\n",
      "\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 10485784\n",
      "overflow length: 28521267\n",
      "Reading vocab from file \"data/vocab.txt\"...loaded 23261 words.\n",
      "Building lookup table...table contains 51813977 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[19G900000\u001b[19G1000000\u001b[19G1100000\u001b[19G1200000\u001b[19G1300000\u001b[19G1400000\u001b[19G1500000\u001b[19G1600000\u001b[19G1700000\u001b[19G1800000\u001b[19G1900000\u001b[19G2000000\u001b[19G2100000\u001b[19G2200000\u001b[19G2300000\u001b[19G2400000\u001b[19G2500000\u001b[19G2600000\u001b[19G2700000\u001b[19G2800000\u001b[19G2900000\u001b[19G3000000\u001b[19G3100000\u001b[19G3200000\u001b[19G3300000\u001b[19G3400000\u001b[19G3500000\u001b[19G3600000\u001b[19G3700000\u001b[19G3800000\u001b[19G3900000\u001b[19G4000000\u001b[19G4100000\u001b[19G4200000\u001b[19G4300000\u001b[19G4400000\u001b[19G4500000\u001b[19G4600000\u001b[19G4700000\u001b[19G4800000\u001b[19G4900000\u001b[19G5000000\u001b[19G5100000\u001b[19G5200000\u001b[19G5300000\u001b[19G5400000\u001b[19G5500000\u001b[19G5600000\u001b[19G5700000\u001b[19G5800000\u001b[19G5900000\u001b[19G6000000\u001b[19G6100000\u001b[19G6200000\u001b[19G6300000\u001b[19G6400000\u001b[19G6500000\u001b[19G6600000\u001b[19G6700000\u001b[19G6800000\u001b[19G6900000\u001b[19G7000000\u001b[19G7100000\u001b[19G7200000\u001b[19G7300000\u001b[0GProcessed 7359817 tokens.\n",
      "Writing cooccurrences to disk........2 files in total.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[39G2900000 lines.\u001b[39G3000000 lines.\u001b[39G3100000 lines.\u001b[39G3200000 lines.\u001b[39G3300000 lines.\u001b[39G3400000 lines.\u001b[39G3500000 lines.\u001b[39G3600000 lines.\u001b[39G3700000 lines.\u001b[39G3800000 lines.\u001b[39G3900000 lines.\u001b[39G4000000 lines.\u001b[39G4100000 lines.\u001b[39G4200000 lines.\u001b[39G4300000 lines.\u001b[39G4400000 lines.\u001b[39G4500000 lines.\u001b[39G4600000 lines.\u001b[39G4700000 lines.\u001b[39G4800000 lines.\u001b[39G4900000 lines.\u001b[39G5000000 lines.\u001b[39G5100000 lines.\u001b[39G5200000 lines.\u001b[39G5300000 lines.\u001b[39G5400000 lines.\u001b[39G5500000 lines.\u001b[39G5600000 lines.\u001b[39G5700000 lines.\u001b[39G5800000 lines.\u001b[39G5900000 lines.\u001b[39G6000000 lines.\u001b[39G6100000 lines.\u001b[39G6200000 lines.\u001b[39G6300000 lines.\u001b[39G6400000 lines.\u001b[39G6500000 lines.\u001b[39G6600000 lines.\u001b[39G6700000 lines.\u001b[39G6800000 lines.\u001b[39G6900000 lines.\u001b[39G7000000 lines.\u001b[39G7100000 lines.\u001b[39G7200000 lines.\u001b[39G7300000 lines.\u001b[39G7400000 lines.\u001b[39G7500000 lines.\u001b[39G7600000 lines.\u001b[39G7700000 lines.\u001b[39G7800000 lines.\u001b[39G7900000 lines.\u001b[39G8000000 lines.\u001b[39G8100000 lines.\u001b[39G8200000 lines.\u001b[39G8300000 lines.\u001b[39G8400000 lines.\u001b[39G8500000 lines.\u001b[39G8600000 lines.\u001b[39G8700000 lines.\u001b[39G8800000 lines.\u001b[39G8900000 lines.\u001b[39G9000000 lines.\u001b[39G9100000 lines.\u001b[39G9200000 lines.\u001b[39G9300000 lines.\u001b[39G9400000 lines.\u001b[39G9500000 lines.\u001b[39G9600000 lines.\u001b[39G9700000 lines.\u001b[39G9800000 lines.\u001b[39G9900000 lines.\u001b[39G10000000 lines.\u001b[39G10100000 lines.\u001b[39G10200000 lines.\u001b[39G10300000 lines.\u001b[39G10400000 lines.\u001b[39G10500000 lines.\u001b[39G10600000 lines.\u001b[39G10700000 lines.\u001b[39G10800000 lines.\u001b[39G10900000 lines.\u001b[39G11000000 lines.\u001b[39G11100000 lines.\u001b[39G11200000 lines.\u001b[39G11300000 lines.\u001b[39G11400000 lines.\u001b[39G11500000 lines.\u001b[39G11600000 lines.\u001b[39G11700000 lines.\u001b[39G11800000 lines.\u001b[39G11900000 lines.\u001b[39G12000000 lines.\u001b[39G12100000 lines.\u001b[39G12200000 lines.\u001b[39G12300000 lines.\u001b[39G12400000 lines.\u001b[39G12500000 lines.\u001b[39G12600000 lines.\u001b[39G12700000 lines.\u001b[39G12800000 lines.\u001b[39G12900000 lines.\u001b[39G13000000 lines.\u001b[39G13100000 lines.\u001b[39G13200000 lines.\u001b[39G13300000 lines.\u001b[39G13400000 lines.\u001b[39G13500000 lines.\u001b[39G13600000 lines.\u001b[39G13700000 lines.\u001b[39G13800000 lines.\u001b[39G13900000 lines.\u001b[39G14000000 lines.\u001b[39G14100000 lines.\u001b[39G14200000 lines.\u001b[39G14300000 lines.\u001b[39G14400000 lines.\u001b[39G14500000 lines.\u001b[39G14600000 lines.\u001b[39G14700000 lines.\u001b[39G14800000 lines.\u001b[39G14900000 lines.\u001b[39G15000000 lines.\u001b[39G15100000 lines.\u001b[39G15200000 lines.\u001b[39G15300000 lines.\u001b[39G15400000 lines.\u001b[39G15500000 lines.\u001b[39G15600000 lines.\u001b[39G15700000 lines.\u001b[39G15800000 lines.\u001b[39G15900000 lines.\u001b[39G16000000 lines.\u001b[39G16100000 lines.\u001b[39G16200000 lines.\u001b[39G16300000 lines.\u001b[39G16400000 lines.\u001b[39G16500000 lines.\u001b[39G16600000 lines.\u001b[39G16700000 lines.\u001b[39G16800000 lines.\u001b[0GMerging cooccurrence files: processed 16816576 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gloVe/build/cooccur -vocab-file data/vocab.txt < data/non_aggressive_comments_plain.txt > data/non_aggressive_comments_cooccurrences.bin\n",
    "!gloVe/build/cooccur -vocab-file data/vocab.txt < data/aggressive_comments_plain.txt > data/aggressive_comments_cooccurrences.bin\n",
    "!gloVe/build/cooccur -vocab-file data/vocab.txt < data/all_comments_plain.txt > data/all_comments_cooccurrences.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed 1580467654\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 15858731 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G15858731 lines.\u001b[0GMerging temp files: processed 15858731 lines.\n",
      "\n",
      "Using random seed 1580467661\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 2927335 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G2927335 lines.\u001b[0GMerging temp files: processed 2927335 lines.\n",
      "\n",
      "Using random seed 1580467662\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 16816576 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G16816576 lines.\u001b[0GMerging temp files: processed 16816576 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gloVe/build/shuffle -verbose 2 -memory 4 < data/non_aggressive_comments_cooccurrences.bin > data/non_aggressive_comments_cooccurrences_shuffled.bin\n",
    "!gloVe/build/shuffle -verbose 2 -memory 4 < data/aggressive_comments_cooccurrences.bin > data/aggressive_comments_cooccurrences_shuffled.bin\n",
    "!gloVe/build/shuffle -verbose 2 -memory 4 < data/all_comments_cooccurrences.bin > data/all_comments_cooccurrences_shuffled.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL\n",
      "Read 15858731 lines.\n",
      "Initializing parameters...Using random seed 1580467674\n",
      "done.\n",
      "vector size: 32\n",
      "vocab size: 23261\n",
      "x_max: 100.000000\n",
      "alpha: 0.750000\n",
      "01/31/20 - 11:47.56AM, iter: 001, cost: 0.021714\n",
      "01/31/20 - 11:47.57AM, iter: 002, cost: 0.014477\n",
      "01/31/20 - 11:47.58AM, iter: 003, cost: 0.012972\n",
      "01/31/20 - 11:48.00AM, iter: 004, cost: 0.011847\n",
      "01/31/20 - 11:48.01AM, iter: 005, cost: 0.010900\n",
      "01/31/20 - 11:48.03AM, iter: 006, cost: 0.010162\n",
      "01/31/20 - 11:48.05AM, iter: 007, cost: 0.009618\n",
      "01/31/20 - 11:48.06AM, iter: 008, cost: 0.009218\n",
      "01/31/20 - 11:48.08AM, iter: 009, cost: 0.008911\n",
      "01/31/20 - 11:48.09AM, iter: 010, cost: 0.008664\n",
      "01/31/20 - 11:48.11AM, iter: 011, cost: 0.008464\n",
      "01/31/20 - 11:48.12AM, iter: 012, cost: 0.008297\n",
      "01/31/20 - 11:48.14AM, iter: 013, cost: 0.008155\n",
      "01/31/20 - 11:48.16AM, iter: 014, cost: 0.008031\n",
      "01/31/20 - 11:48.18AM, iter: 015, cost: 0.007925\n",
      "01/31/20 - 11:48.19AM, iter: 016, cost: 0.007832\n",
      "01/31/20 - 11:48.21AM, iter: 017, cost: 0.007749\n",
      "01/31/20 - 11:48.23AM, iter: 018, cost: 0.007675\n",
      "01/31/20 - 11:48.24AM, iter: 019, cost: 0.007609\n",
      "01/31/20 - 11:48.26AM, iter: 020, cost: 0.007550\n",
      "01/31/20 - 11:48.27AM, iter: 021, cost: 0.007496\n",
      "01/31/20 - 11:48.29AM, iter: 022, cost: 0.007444\n",
      "01/31/20 - 11:48.31AM, iter: 023, cost: 0.007399\n",
      "01/31/20 - 11:48.33AM, iter: 024, cost: 0.007355\n",
      "01/31/20 - 11:48.34AM, iter: 025, cost: 0.007315\n",
      "TRAINING MODEL\n",
      "Read 2927335 lines.\n",
      "Initializing parameters...Using random seed 1580467715\n",
      "done.\n",
      "vector size: 32\n",
      "vocab size: 23261\n",
      "x_max: 100.000000\n",
      "alpha: 0.750000\n",
      "01/31/20 - 11:48.35AM, iter: 001, cost: 0.028593\n",
      "01/31/20 - 11:48.35AM, iter: 002, cost: 0.018337\n",
      "01/31/20 - 11:48.36AM, iter: 003, cost: 0.015296\n",
      "01/31/20 - 11:48.36AM, iter: 004, cost: 0.013999\n",
      "01/31/20 - 11:48.36AM, iter: 005, cost: 0.012851\n",
      "01/31/20 - 11:48.37AM, iter: 006, cost: 0.011837\n",
      "01/31/20 - 11:48.37AM, iter: 007, cost: 0.011001\n",
      "01/31/20 - 11:48.37AM, iter: 008, cost: 0.010294\n",
      "01/31/20 - 11:48.37AM, iter: 009, cost: 0.009703\n",
      "01/31/20 - 11:48.38AM, iter: 010, cost: 0.009219\n",
      "01/31/20 - 11:48.38AM, iter: 011, cost: 0.008819\n",
      "01/31/20 - 11:48.38AM, iter: 012, cost: 0.008474\n",
      "01/31/20 - 11:48.39AM, iter: 013, cost: 0.008188\n",
      "01/31/20 - 11:48.39AM, iter: 014, cost: 0.007939\n",
      "01/31/20 - 11:48.39AM, iter: 015, cost: 0.007723\n",
      "01/31/20 - 11:48.39AM, iter: 016, cost: 0.007535\n",
      "01/31/20 - 11:48.40AM, iter: 017, cost: 0.007371\n",
      "01/31/20 - 11:48.40AM, iter: 018, cost: 0.007223\n",
      "01/31/20 - 11:48.40AM, iter: 019, cost: 0.007095\n",
      "01/31/20 - 11:48.41AM, iter: 020, cost: 0.006977\n",
      "01/31/20 - 11:48.41AM, iter: 021, cost: 0.006871\n",
      "01/31/20 - 11:48.41AM, iter: 022, cost: 0.006771\n",
      "01/31/20 - 11:48.41AM, iter: 023, cost: 0.006684\n",
      "01/31/20 - 11:48.42AM, iter: 024, cost: 0.006603\n",
      "01/31/20 - 11:48.42AM, iter: 025, cost: 0.006523\n",
      "TRAINING MODEL\n",
      "Read 16816576 lines.\n",
      "Initializing parameters...Using random seed 1580467723\n",
      "done.\n",
      "vector size: 32\n",
      "vocab size: 23261\n",
      "x_max: 100.000000\n",
      "alpha: 0.750000\n",
      "01/31/20 - 11:48.45AM, iter: 001, cost: 0.023016\n",
      "01/31/20 - 11:48.47AM, iter: 002, cost: 0.015396\n",
      "01/31/20 - 11:48.48AM, iter: 003, cost: 0.013727\n",
      "01/31/20 - 11:48.50AM, iter: 004, cost: 0.012448\n",
      "01/31/20 - 11:48.52AM, iter: 005, cost: 0.011374\n",
      "01/31/20 - 11:48.54AM, iter: 006, cost: 0.010554\n",
      "01/31/20 - 11:48.56AM, iter: 007, cost: 0.009967\n",
      "01/31/20 - 11:48.57AM, iter: 008, cost: 0.009533\n",
      "01/31/20 - 11:48.59AM, iter: 009, cost: 0.009202\n",
      "01/31/20 - 11:49.01AM, iter: 010, cost: 0.008943\n",
      "01/31/20 - 11:49.03AM, iter: 011, cost: 0.008729\n",
      "01/31/20 - 11:49.05AM, iter: 012, cost: 0.008554\n",
      "01/31/20 - 11:49.06AM, iter: 013, cost: 0.008407\n",
      "01/31/20 - 11:49.08AM, iter: 014, cost: 0.008280\n",
      "01/31/20 - 11:49.10AM, iter: 015, cost: 0.008171\n",
      "01/31/20 - 11:49.12AM, iter: 016, cost: 0.008077\n",
      "01/31/20 - 11:49.14AM, iter: 017, cost: 0.007990\n",
      "01/31/20 - 11:49.15AM, iter: 018, cost: 0.007917\n",
      "01/31/20 - 11:49.17AM, iter: 019, cost: 0.007847\n",
      "01/31/20 - 11:49.19AM, iter: 020, cost: 0.007786\n",
      "01/31/20 - 11:49.21AM, iter: 021, cost: 0.007730\n",
      "01/31/20 - 11:49.22AM, iter: 022, cost: 0.007679\n",
      "01/31/20 - 11:49.24AM, iter: 023, cost: 0.007633\n",
      "01/31/20 - 11:49.26AM, iter: 024, cost: 0.007589\n",
      "01/31/20 - 11:49.27AM, iter: 025, cost: 0.007548\n"
     ]
    }
   ],
   "source": [
    "!gloVe/build/glove -input-file data/non_aggressive_comments_cooccurrences_shuffled.bin -vocab-file data/vocab.txt -save-file data/non_aggressive_comments_vec -verbose 2 -vector-size 32 -threads 8 -alpha 0.75 -x-max 100.0 -eta 0.05 -binary 0 -model 2\n",
    "!gloVe/build/glove -input-file data/aggressive_comments_cooccurrences_shuffled.bin -vocab-file data/vocab.txt -save-file data/aggressive_comments_vec -verbose 2 -vector-size 32 -threads 8 -alpha 0.75 -x-max 100.0 -eta 0.05 -binary 0 -model 2\n",
    "!gloVe/build/glove -input-file data/all_comments_cooccurrences_shuffled.bin -vocab-file data/vocab.txt -save-file data/all_comments_vec -verbose 2 -vector-size 32 -threads 8 -alpha 0.75 -x-max 100.0 -eta 0.05 -binary 0 -model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "import numpy as np\n",
    "\n",
    "with open(\"data/vocab.txt\") as fin:\n",
    "    vocab,_ = zip(*map(lambda x: x.split(\" \"), fin))\n",
    "vocab = sorted(list(vocab) + [\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b435b7d181df4a99b8143baa57390af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d3c699dd3f4437b933a0a136526251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4de70ea4514921a23dc6f212b0d799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def index(a, x):\n",
    "    'Locate the leftmost value exactly equal to x'\n",
    "    i = bisect_left(a, x)\n",
    "    if i != len(a) and a[i] == x:\n",
    "        return i\n",
    "    raise ValueError\n",
    "\n",
    "def load_embedding(path_, vocab):\n",
    "    dim = 32\n",
    "    mat = np.empty((len(vocab), dim))\n",
    "    \n",
    "    with open(path_) as fin:\n",
    "        for row in tqdm(fin):\n",
    "            splitted = row.replace(\"\\n\", \"\").split(\" \")\n",
    "            key, vec = splitted[0], splitted[1:]\n",
    "            mat[index(vocab, key)] = vec\n",
    "            \n",
    "    return mat\n",
    "            \n",
    "all_vec = load_embedding(\"data/all_comments_vec.txt\", vocab)\n",
    "aggressive_vec = load_embedding(\"data/aggressive_comments_vec.txt\", vocab)\n",
    "non_aggressive_vec = load_embedding(\"data/non_aggressive_comments_vec.txt\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1098bea68cb94e2391b488411d670665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21724.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "misses/hits 0.08105129530252014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbed3368a9c418a95fb0b6ea6253943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7242.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "misses/hits 0.08537588390176032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.97      0.94      6327\n",
      "        True       0.65      0.45      0.53       915\n",
      "\n",
      "    accuracy                           0.90      7242\n",
      "   macro avg       0.79      0.71      0.74      7242\n",
      "weighted avg       0.89      0.90      0.89      7242\n",
      "\n",
      "aggressive_vec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607cba0bf852486bb667fa195b565d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21724.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "misses/hits 0.08105129530252014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc7aa12e24449559ee3d1ed7d7b78c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7242.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "misses/hits 0.08537588390176032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      0.98      0.94      6327\n",
      "        True       0.69      0.34      0.46       915\n",
      "\n",
      "    accuracy                           0.90      7242\n",
      "   macro avg       0.80      0.66      0.70      7242\n",
      "weighted avg       0.88      0.90      0.88      7242\n",
      "\n",
      "non_aggressive_vec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d98c2f3c504a02aea5b6af5b775d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21724.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "misses/hits 0.08105129530252014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19347fb700d4c26a3918fbfadecdfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7242.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "misses/hits 0.08537588390176032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.95      0.93      6327\n",
      "        True       0.55      0.42      0.47       915\n",
      "\n",
      "    accuracy                           0.88      7242\n",
      "   macro avg       0.73      0.68      0.70      7242\n",
      "weighted avg       0.87      0.88      0.88      7242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def embed_X(X, vocab, embedding, verbose=True):\n",
    "    embedded_X = np.zeros((len(X), embedding.shape[1]))\n",
    "    num_misses = 0\n",
    "    num_hits = 0\n",
    "    for icomment, comment in tqdm(enumerate(tokenizer.pipe(X)), total=len(X)):\n",
    "        for token in comment:\n",
    "            try:\n",
    "                embedded_X[icomment] += embedding[index(vocab, token.text.replace(\"\\n\", \"\\t\"))]\n",
    "                num_hits += 1\n",
    "            except ValueError:\n",
    "                num_misses += 1\n",
    "                if verbose:\n",
    "                    print(\"{} not in vocab\".format(token))\n",
    "        embedded_X[icomment] /= len(comment)\n",
    "    \n",
    "    print(\"misses/hits {}\".format(num_misses/num_hits))\n",
    "    return embedded_X\n",
    "\n",
    "def train(X,y):\n",
    "      \n",
    "    clf = KNeighborsClassifier()\n",
    "    \n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "def evaluate(clf, X, y_true):\n",
    "    y_pred = clf.predict(X)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"all\")\n",
    "\n",
    "clf = train(embed_X(X_train, vocab, all_vec, verbose=False), y_train)\n",
    "evaluate(clf, embed_X(X_test, vocab, all_vec, verbose=False), y_test)\n",
    "\n",
    "print(\"aggressive_vec\")\n",
    "\n",
    "clf = train(embed_X(X_train, vocab, aggressive_vec, verbose=False), y_train)\n",
    "evaluate(clf, embed_X(X_test, vocab, aggressive_vec, verbose=False), y_test)\n",
    "\n",
    "print(\"non_aggressive_vec\")\n",
    "\n",
    "clf = train(embed_X(X_train, vocab, non_aggressive_vec, verbose=False), y_train)\n",
    "evaluate(clf, embed_X(X_test, vocab, non_aggressive_vec, verbose=False), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wh4",
   "language": "python",
   "name": "wh4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
