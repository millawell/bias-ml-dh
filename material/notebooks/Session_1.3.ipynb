{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Collecting git+https://github.com/millawell/bias-ml-dh.git#subdirectory=material/notebooks/bias_ml_dh_utils\n  Cloning https://github.com/millawell/bias-ml-dh.git to /private/var/folders/jp/c1yk1wvn6x58ftj04zg1z8680000gn/T/pip-req-build-5le5duur\n  Running command git clone -q https://github.com/millawell/bias-ml-dh.git /private/var/folders/jp/c1yk1wvn6x58ftj04zg1z8680000gn/T/pip-req-build-5le5duur\nRequirement already satisfied (use --upgrade to upgrade): bias-ml-dh-utils==0.1 from git+https://github.com/millawell/bias-ml-dh.git#subdirectory=material/notebooks/bias_ml_dh_utils in /Users/davidlassner/Envs/wh4/lib/python3.7/site-packages\nBuilding wheels for collected packages: bias-ml-dh-utils\n  Building wheel for bias-ml-dh-utils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for bias-ml-dh-utils: filename=bias_ml_dh_utils-0.1-py3-none-any.whl size=2211 sha256=40f6b477a4a98047430dc86aaf261a00b2c5bd7da73f4dec8c12c5054a9605e9\n  Stored in directory: /private/var/folders/jp/c1yk1wvn6x58ftj04zg1z8680000gn/T/pip-ephem-wheel-cache-n3k8tx3l/wheels/d4/8d/af/a27ef01a2dc2a7313cfcbb128b906b77e986b55a6ebd3d52a7\nSuccessfully built bias-ml-dh-utils\nRequirement already up-to-date: tqdm in /Users/davidlassner/Envs/wh4/lib/python3.7/site-packages (4.43.0)\nfatal: destination path 'GloVe' already exists and is not an empty directory.\n/Users/davidlassner/research/conferences_and_papers/workshops/2020 DHd/website/material/notebooks/GloVe\nmkdir -p build\n/Users/davidlassner/research/conferences_and_papers/workshops/2020 DHd/website/material/notebooks\n"
    }
   ],
   "source": [
    "!pip install git+https://github.com/millawell/bias-ml-dh.git#subdirectory=material/notebooks/bias_ml_dh_utils\n",
    "!pip install --upgrade tqdm\n",
    "!git clone https://github.com/stanfordnlp/GloVe.git\n",
    "%cd GloVe\n",
    "!make\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the aggressive wikipedia comments data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "mkdir: data: File exists\n--2020-02-29 15:25:34--  https://ndownloader.figshare.com/files/7394506\nResolving ndownloader.figshare.com (ndownloader.figshare.com)...18.203.184.13, 54.246.162.108, 34.252.157.212, ...\nConnecting to ndownloader.figshare.com (ndownloader.figshare.com)|18.203.184.13|:443...connected.\nHTTP request sent, awaiting response...302 Found\nLocation: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/7394506/aggression_annotations.tsv [following]\n--2020-02-29 15:25:34--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/7394506/aggression_annotations.tsv\nResolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.56.3\nConnecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.56.3|:443...connected.\nHTTP request sent, awaiting response...200 OK\nLength: 30627328 (29M) [binary/octet-stream]\nSaving to: ‘agression_annotation.tsv’\n\nagression_annotatio 100%[===================>]  29.21M  41.1MB/s    in 0.7s    \n\n2020-02-29 15:25:35 (41.1 MB/s) - ‘agression_annotation.tsv’ saved [30627328/30627328]\n\n--2020-02-29 15:25:35--  https://ndownloader.figshare.com/files/7038038\nResolving ndownloader.figshare.com (ndownloader.figshare.com)... 54.246.162.108, 34.252.157.212, 18.203.5.169, ...\nConnecting to ndownloader.figshare.com (ndownloader.figshare.com)|54.246.162.108|:443...connected.\nHTTP request sent, awaiting response...302 Found\nLocation: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/7038038/aggression_annotated_comments.tsv [following]\n--2020-02-29 15:25:35--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/7038038/aggression_annotated_comments.tsv\nResolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.56.3\nConnecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.56.3|:443... connected.\nHTTP request sent, awaiting response...200 OK\nLength: 58090178 (55M) [binary/octet-stream]\nSaving to: ‘aggression_annotated_comments.tsv’\n\naggression_annotate 100%[===================>]  55.40M  48.2MB/s    in 1.1s    \n\n2020-02-29 15:25:37 (48.2 MB/s) - ‘aggression_annotated_comments.tsv’ saved [58090178/58090178]\n\n"
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget https://ndownloader.figshare.com/files/7394506 -O agression_annotation.tsv\n",
    "!wget https://ndownloader.figshare.com/files/7038038 -O aggression_annotated_comments.tsv\n",
    "!mv agression_annotation.tsv data/\n",
    "!mv aggression_annotated_comments.tsv data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "aggression_annotated_comments = pd.read_csv(\"data/aggression_annotated_comments.tsv\", sep=\"\\t\")\n",
    "agression_annotation = pd.read_csv(\"data/agression_annotation.tsv\", sep=\"\\t\")\n",
    "\n",
    "agression_data = pd.merge(aggression_annotated_comments, agression_annotation, on=\"rev_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 115864/115864 [00:59<00:00, 1951.27it/s]\n"
    }
   ],
   "source": [
    "comments = []\n",
    "labels = []\n",
    "\n",
    "def regex_filter(comment):\n",
    "    return comment.replace(\"NEWLINE_TOKEN\", \"\\t\")\n",
    "\n",
    "for rev_id, rev in tqdm(agression_data.groupby(\"rev_id\")):\n",
    "    comments.append(regex_filter(rev.iloc[0].comment))\n",
    "    labels.append(rev.aggression.sum()/len(rev) >.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split    \n",
    "train_embeddings, X_other, embeddings_labels, y_other  = train_test_split(comments, labels, random_state=123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_other, y_other, random_state=456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "storing comments: 100%|██████████| 86898/86898 [01:03<00:00, 1358.15it/s]\n"
    }
   ],
   "source": [
    "with open(\"data/all_comments_plain.txt\", \"w\") as all_fout:\n",
    "    with open(\"data/aggressive_comments_plain.txt\", \"w\") as agg_fout:\n",
    "        with open(\"data/non_aggressive_comments_plain.txt\", \"w\") as non_agg_fout:\n",
    "            for comment, label in tqdm(\n",
    "                zip(tokenizer.pipe(train_embeddings), embeddings_labels),\n",
    "                total=len(train_embeddings),\n",
    "                desc=\"storing comments\"\n",
    "            ):\n",
    "                out_line = \" \".join([t.text.replace(\"\\n\", \"\\t\") for t in comment])\n",
    "                all_fout.write(\"{}\\n\".format(out_line))\n",
    "                if label == 1:\n",
    "                    agg_fout.write(\"{}\\n\".format(out_line))\n",
    "                else:\n",
    "                    non_agg_fout.write(\"{}\\n\".format(out_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "agg_vec_data.tsv\nagg_vec_meta.tsv\naggression_annotated_comments.tsv\naggressive_comments_cooccurrences.bin\naggressive_comments_cooccurrences_shuffled.bin\naggressive_comments_plain.txt\naggressive_comments_vec.txt\nagression_annotation.tsv\nall_comments_cooccurrences.bin\nall_comments_cooccurrences_shuffled.bin\nall_comments_plain.txt\nall_comments_vec.txt\nall_vec_data.tsv\nall_vec_meta.tsv\nnoagg_vec_data.tsv\nnoagg_vec_meta.tsv\nnon_aggressive_comments_cooccurrences.bin\nnon_aggressive_comments_cooccurrences_shuffled.bin\nnon_aggressive_comments_plain.txt\nnon_aggressive_comments_vec.txt\n\u001b[34mtwitter\u001b[m\u001b[m\nvocab.txt\n"
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "common.o      cooccur.o     glove.o       shuffle.o     vocab_count.o\n\u001b[31mcooccur\u001b[m\u001b[m       \u001b[31mglove\u001b[m\u001b[m         \u001b[31mshuffle\u001b[m\u001b[m       \u001b[31mvocab_count\u001b[m\u001b[m\n"
    }
   ],
   "source": [
    "!ls GloVe/build/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[11G100000 tokens.\u001b[11G200000 tokens.\u001b[11G300000 tokens.\u001b[11G400000 tokens.\u001b[11G500000 tokens.\u001b[11G600000 tokens.\u001b[11G700000 tokens.\u001b[11G800000 tokens.\u001b[11G900000 tokens.\u001b[11G1000000 tokens.\u001b[11G1100000 tokens.\u001b[11G1200000 tokens.\u001b[11G1300000 tokens.\u001b[11G1400000 tokens.\u001b[11G1500000 tokens.\u001b[11G1600000 tokens.\u001b[11G1700000 tokens.\u001b[11G1800000 tokens.\u001b[11G1900000 tokens.\u001b[11G2000000 tokens.\u001b[11G2100000 tokens.\u001b[11G2200000 tokens.\u001b[11G2300000 tokens.\u001b[11G2400000 tokens.\u001b[11G2500000 tokens.\u001b[11G2600000 tokens.\u001b[11G2700000 tokens.\u001b[11G2800000 tokens.\u001b[11G2900000 tokens.\u001b[11G3000000 tokens.\u001b[11G3100000 tokens.\u001b[11G3200000 tokens.\u001b[11G3300000 tokens.\u001b[11G3400000 tokens.\u001b[11G3500000 tokens.\u001b[11G3600000 tokens.\u001b[11G3700000 tokens.\u001b[11G3800000 tokens.\u001b[11G3900000 tokens.\u001b[11G4000000 tokens.\u001b[11G4100000 tokens.\u001b[11G4200000 tokens.\u001b[11G4300000 tokens.\u001b[11G4400000 tokens.\u001b[11G4500000 tokens.\u001b[11G4600000 tokens.\u001b[11G4700000 tokens.\u001b[11G4800000 tokens.\u001b[11G4900000 tokens.\u001b[11G5000000 tokens.\u001b[11G5100000 tokens.\u001b[11G5200000 tokens.\u001b[11G5300000 tokens.\u001b[11G5400000 tokens.\u001b[11G5500000 tokens.\u001b[11G5600000 tokens.\u001b[11G5700000 tokens.\u001b[11G5800000 tokens.\u001b[11G5900000 tokens.\u001b[11G6000000 tokens.\u001b[11G6100000 tokens.\u001b[11G6200000 tokens.\u001b[11G6300000 tokens.\u001b[11G6400000 tokens.\u001b[11G6500000 tokens.\u001b[11G6600000 tokens.\u001b[11G6700000 tokens.\u001b[11G6800000 tokens.\u001b[11G6900000 tokens.\u001b[11G7000000 tokens.\u001b[11G7100000 tokens.\u001b[11G7200000 tokens.\u001b[11G7300000 tokens.\u001b[11G7400000 tokens.\u001b[11G7500000 tokens.\u001b[0GProcessed 7504348 tokens.\n",
      "Counted 182750 unique words.\n",
      "Truncating vocabulary at min count 10.\n",
      "Using vocabulary of size 22657.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gloVe/build/vocab_count -max-vocab 25000 -min-count 10 < data/all_comments_plain.txt > data/vocab.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cooccurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 10485784\n",
      "overflow length: 28521267\n",
      "Reading vocab from file \"data/vocab.txt\"...loaded 22657 words.\n",
      "Building lookup table...table contains 51262847 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[19G900000\u001b[19G1000000\u001b[19G1100000\u001b[19G1200000\u001b[19G1300000\u001b[19G1400000\u001b[19G1500000\u001b[19G1600000\u001b[19G1700000\u001b[19G1800000\u001b[19G1900000\u001b[19G2000000\u001b[19G2100000\u001b[19G2200000\u001b[19G2300000\u001b[19G2400000\u001b[19G2500000\u001b[19G2600000\u001b[19G2700000\u001b[19G2800000\u001b[19G2900000\u001b[19G3000000\u001b[19G3100000\u001b[19G3200000\u001b[19G3300000\u001b[19G3400000\u001b[19G3500000\u001b[19G3600000\u001b[19G3700000\u001b[19G3800000\u001b[19G3900000\u001b[19G4000000\u001b[19G4100000\u001b[19G4200000\u001b[19G4300000\u001b[19G4400000\u001b[19G4500000\u001b[19G4600000\u001b[19G4700000\u001b[19G4800000\u001b[19G4900000\u001b[19G5000000\u001b[19G5100000\u001b[19G5200000\u001b[19G5300000\u001b[19G5400000\u001b[19G5500000\u001b[19G5600000\u001b[19G5700000\u001b[19G5800000\u001b[19G5900000\u001b[19G6000000\u001b[19G6100000\u001b[19G6200000\u001b[19G6300000\u001b[19G6400000\u001b[19G6500000\u001b[19G6600000\u001b[0GProcessed 6668532 tokens.\n",
      "Writing cooccurrences to disk........2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[39G2900000 lines.\u001b[39G3000000 lines.\u001b[39G3100000 lines.\u001b[39G3200000 lines.\u001b[39G3300000 lines.\u001b[39G3400000 lines.\u001b[39G3500000 lines.\u001b[39G3600000 lines.\u001b[39G3700000 lines.\u001b[39G3800000 lines.\u001b[39G3900000 lines.\u001b[39G4000000 lines.\u001b[39G4100000 lines.\u001b[39G4200000 lines.\u001b[39G4300000 lines.\u001b[39G4400000 lines.\u001b[39G4500000 lines.\u001b[39G4600000 lines.\u001b[39G4700000 lines.\u001b[39G4800000 lines.\u001b[39G4900000 lines.\u001b[39G5000000 lines.\u001b[39G5100000 lines.\u001b[39G5200000 lines.\u001b[39G5300000 lines.\u001b[39G5400000 lines.\u001b[39G5500000 lines.\u001b[39G5600000 lines.\u001b[39G5700000 lines.\u001b[39G5800000 lines.\u001b[39G5900000 lines.\u001b[39G6000000 lines.\u001b[39G6100000 lines.\u001b[39G6200000 lines.\u001b[39G6300000 lines.\u001b[39G6400000 lines.\u001b[39G6500000 lines.\u001b[39G6600000 lines.\u001b[39G6700000 lines.\u001b[39G6800000 lines.\u001b[39G6900000 lines.\u001b[39G7000000 lines.\u001b[39G7100000 lines.\u001b[39G7200000 lines.\u001b[39G7300000 lines.\u001b[39G7400000 lines.\u001b[39G7500000 lines.\u001b[39G7600000 lines.\u001b[39G7700000 lines.\u001b[39G7800000 lines.\u001b[39G7900000 lines.\u001b[39G8000000 lines.\u001b[39G8100000 lines.\u001b[39G8200000 lines.\u001b[39G8300000 lines.\u001b[39G8400000 lines.\u001b[39G8500000 lines.\u001b[39G8600000 lines.\u001b[39G8700000 lines.\u001b[39G8800000 lines.\u001b[39G8900000 lines.\u001b[39G9000000 lines.\u001b[39G9100000 lines.\u001b[39G9200000 lines.\u001b[39G9300000 lines.\u001b[39G9400000 lines.\u001b[39G9500000 lines.\u001b[39G9600000 lines.\u001b[39G9700000 lines.\u001b[39G9800000 lines.\u001b[39G9900000 lines.\u001b[39G10000000 lines.\u001b[39G10100000 lines.\u001b[39G10200000 lines.\u001b[39G10300000 lines.\u001b[39G10400000 lines.\u001b[39G10500000 lines.\u001b[39G10600000 lines.\u001b[39G10700000 lines.\u001b[39G10800000 lines.\u001b[39G10900000 lines.\u001b[39G11000000 lines.\u001b[39G11100000 lines.\u001b[39G11200000 lines.\u001b[39G11300000 lines.\u001b[39G11400000 lines.\u001b[39G11500000 lines.\u001b[39G11600000 lines.\u001b[39G11700000 lines.\u001b[39G11800000 lines.\u001b[39G11900000 lines.\u001b[39G12000000 lines.\u001b[39G12100000 lines.\u001b[39G12200000 lines.\u001b[39G12300000 lines.\u001b[39G12400000 lines.\u001b[39G12500000 lines.\u001b[39G12600000 lines.\u001b[39G12700000 lines.\u001b[39G12800000 lines.\u001b[39G12900000 lines.\u001b[39G13000000 lines.\u001b[39G13100000 lines.\u001b[39G13200000 lines.\u001b[39G13300000 lines.\u001b[39G13400000 lines.\u001b[39G13500000 lines.\u001b[39G13600000 lines.\u001b[39G13700000 lines.\u001b[39G13800000 lines.\u001b[39G13900000 lines.\u001b[39G14000000 lines.\u001b[39G14100000 lines.\u001b[39G14200000 lines.\u001b[39G14300000 lines.\u001b[39G14400000 lines.\u001b[39G14500000 lines.\u001b[39G14600000 lines.\u001b[39G14700000 lines.\u001b[39G14800000 lines.\u001b[39G14900000 lines.\u001b[39G15000000 lines.\u001b[39G15100000 lines.\u001b[39G15200000 lines.\u001b[39G15300000 lines.\u001b[39G15400000 lines.\u001b[39G15500000 lines.\u001b[39G15600000 lines.\u001b[0GMerging cooccurrence files: processed 15671104 lines.\n",
      "\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 10485784\n",
      "overflow length: 28521267\n",
      "Reading vocab from file \"data/vocab.txt\"...loaded 22657 words.\n",
      "Building lookup table...table contains 51262847 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[0GProcessed 835816 tokens.\n",
      "Writing cooccurrences to disk........2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[0GMerging cooccurrence files: processed 2843613 lines.\n",
      "\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 10485784\n",
      "overflow length: 28521267\n",
      "Reading vocab from file \"data/vocab.txt\"...loaded 22657 words.\n",
      "Building lookup table...table contains 51262847 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[19G900000\u001b[19G1000000\u001b[19G1100000\u001b[19G1200000\u001b[19G1300000\u001b[19G1400000\u001b[19G1500000\u001b[19G1600000\u001b[19G1700000\u001b[19G1800000\u001b[19G1900000\u001b[19G2000000\u001b[19G2100000\u001b[19G2200000\u001b[19G2300000\u001b[19G2400000\u001b[19G2500000\u001b[19G2600000\u001b[19G2700000\u001b[19G2800000\u001b[19G2900000\u001b[19G3000000\u001b[19G3100000\u001b[19G3200000\u001b[19G3300000\u001b[19G3400000\u001b[19G3500000\u001b[19G3600000\u001b[19G3700000\u001b[19G3800000\u001b[19G3900000\u001b[19G4000000\u001b[19G4100000\u001b[19G4200000\u001b[19G4300000\u001b[19G4400000\u001b[19G4500000\u001b[19G4600000\u001b[19G4700000\u001b[19G4800000\u001b[19G4900000\u001b[19G5000000\u001b[19G5100000\u001b[19G5200000\u001b[19G5300000\u001b[19G5400000\u001b[19G5500000\u001b[19G5600000\u001b[19G5700000\u001b[19G5800000\u001b[19G5900000\u001b[19G6000000\u001b[19G6100000\u001b[19G6200000\u001b[19G6300000\u001b[19G6400000\u001b[19G6500000\u001b[19G6600000\u001b[19G6700000\u001b[19G6800000\u001b[19G6900000\u001b[19G7000000\u001b[19G7100000\u001b[19G7200000\u001b[19G7300000\u001b[19G7400000\u001b[19G7500000\u001b[0GProcessed 7504348 tokens.\n",
      "Writing cooccurrences to disk........2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[39G2900000 lines.\u001b[39G3000000 lines.\u001b[39G3100000 lines.\u001b[39G3200000 lines.\u001b[39G3300000 lines.\u001b[39G3400000 lines.\u001b[39G3500000 lines.\u001b[39G3600000 lines.\u001b[39G3700000 lines.\u001b[39G3800000 lines.\u001b[39G3900000 lines.\u001b[39G4000000 lines.\u001b[39G4100000 lines.\u001b[39G4200000 lines.\u001b[39G4300000 lines.\u001b[39G4400000 lines.\u001b[39G4500000 lines.\u001b[39G4600000 lines.\u001b[39G4700000 lines.\u001b[39G4800000 lines.\u001b[39G4900000 lines.\u001b[39G5000000 lines.\u001b[39G5100000 lines.\u001b[39G5200000 lines.\u001b[39G5300000 lines.\u001b[39G5400000 lines.\u001b[39G5500000 lines.\u001b[39G5600000 lines.\u001b[39G5700000 lines.\u001b[39G5800000 lines.\u001b[39G5900000 lines.\u001b[39G6000000 lines.\u001b[39G6100000 lines.\u001b[39G6200000 lines.\u001b[39G6300000 lines.\u001b[39G6400000 lines.\u001b[39G6500000 lines.\u001b[39G6600000 lines.\u001b[39G6700000 lines.\u001b[39G6800000 lines.\u001b[39G6900000 lines.\u001b[39G7000000 lines.\u001b[39G7100000 lines.\u001b[39G7200000 lines.\u001b[39G7300000 lines.\u001b[39G7400000 lines.\u001b[39G7500000 lines.\u001b[39G7600000 lines.\u001b[39G7700000 lines.\u001b[39G7800000 lines.\u001b[39G7900000 lines.\u001b[39G8000000 lines.\u001b[39G8100000 lines.\u001b[39G8200000 lines.\u001b[39G8300000 lines.\u001b[39G8400000 lines.\u001b[39G8500000 lines.\u001b[39G8600000 lines.\u001b[39G8700000 lines.\u001b[39G8800000 lines.\u001b[39G8900000 lines.\u001b[39G9000000 lines.\u001b[39G9100000 lines.\u001b[39G9200000 lines.\u001b[39G9300000 lines.\u001b[39G9400000 lines.\u001b[39G9500000 lines.\u001b[39G9600000 lines.\u001b[39G9700000 lines.\u001b[39G9800000 lines.\u001b[39G9900000 lines.\u001b[39G10000000 lines.\u001b[39G10100000 lines.\u001b[39G10200000 lines.\u001b[39G10300000 lines.\u001b[39G10400000 lines.\u001b[39G10500000 lines.\u001b[39G10600000 lines.\u001b[39G10700000 lines.\u001b[39G10800000 lines.\u001b[39G10900000 lines.\u001b[39G11000000 lines.\u001b[39G11100000 lines.\u001b[39G11200000 lines.\u001b[39G11300000 lines.\u001b[39G11400000 lines.\u001b[39G11500000 lines.\u001b[39G11600000 lines.\u001b[39G11700000 lines.\u001b[39G11800000 lines.\u001b[39G11900000 lines.\u001b[39G12000000 lines.\u001b[39G12100000 lines.\u001b[39G12200000 lines.\u001b[39G12300000 lines.\u001b[39G12400000 lines.\u001b[39G12500000 lines.\u001b[39G12600000 lines.\u001b[39G12700000 lines.\u001b[39G12800000 lines.\u001b[39G12900000 lines.\u001b[39G13000000 lines.\u001b[39G13100000 lines.\u001b[39G13200000 lines.\u001b[39G13300000 lines.\u001b[39G13400000 lines.\u001b[39G13500000 lines.\u001b[39G13600000 lines.\u001b[39G13700000 lines.\u001b[39G13800000 lines.\u001b[39G13900000 lines.\u001b[39G14000000 lines.\u001b[39G14100000 lines.\u001b[39G14200000 lines.\u001b[39G14300000 lines.\u001b[39G14400000 lines.\u001b[39G14500000 lines.\u001b[39G14600000 lines.\u001b[39G14700000 lines.\u001b[39G14800000 lines.\u001b[39G14900000 lines.\u001b[39G15000000 lines.\u001b[39G15100000 lines.\u001b[39G15200000 lines.\u001b[39G15300000 lines.\u001b[39G15400000 lines.\u001b[39G15500000 lines.\u001b[39G15600000 lines.\u001b[39G15700000 lines.\u001b[39G15800000 lines.\u001b[39G15900000 lines.\u001b[39G16000000 lines.\u001b[39G16100000 lines.\u001b[39G16200000 lines.\u001b[39G16300000 lines.\u001b[39G16400000 lines.\u001b[39G16500000 lines.\u001b[0GMerging cooccurrence files: processed 16595758 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gloVe/build/cooccur -vocab-file data/vocab.txt < data/non_aggressive_comments_plain.txt > data/non_aggressive_comments_cooccurrences.bin\n",
    "!gloVe/build/cooccur -vocab-file data/vocab.txt < data/aggressive_comments_plain.txt > data/aggressive_comments_cooccurrences.bin\n",
    "!gloVe/build/cooccur -vocab-file data/vocab.txt < data/all_comments_plain.txt > data/all_comments_cooccurrences.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed 1580483054\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 15671104 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G15671104 lines.\u001b[0GMerging temp files: processed 15671104 lines.\n",
      "\n",
      "Using random seed 1580483061\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 2843613 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G2843613 lines.\u001b[0GMerging temp files: processed 2843613 lines.\n",
      "\n",
      "Using random seed 1580483062\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 16595758 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G16595758 lines.\u001b[0GMerging temp files: processed 16595758 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gloVe/build/shuffle -verbose 2 -memory 4 < data/non_aggressive_comments_cooccurrences.bin > data/non_aggressive_comments_cooccurrences_shuffled.bin\n",
    "!gloVe/build/shuffle -verbose 2 -memory 4 < data/aggressive_comments_cooccurrences.bin > data/aggressive_comments_cooccurrences_shuffled.bin\n",
    "!gloVe/build/shuffle -verbose 2 -memory 4 < data/all_comments_cooccurrences.bin > data/all_comments_cooccurrences_shuffled.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL\n",
      "Read 15671104 lines.\n",
      "Initializing parameters...Using random seed 1580483070\n",
      "done.\n",
      "vector size: 32\n",
      "vocab size: 22657\n",
      "x_max: 100.000000\n",
      "alpha: 0.750000\n",
      "01/31/20 - 04:04.33PM, iter: 001, cost: 0.022223\n",
      "01/31/20 - 04:04.34PM, iter: 002, cost: 0.014752\n",
      "01/31/20 - 04:04.36PM, iter: 003, cost: 0.013140\n",
      "01/31/20 - 04:04.37PM, iter: 004, cost: 0.011990\n",
      "01/31/20 - 04:04.39PM, iter: 005, cost: 0.011016\n",
      "01/31/20 - 04:04.41PM, iter: 006, cost: 0.010252\n",
      "01/31/20 - 04:04.42PM, iter: 007, cost: 0.009696\n",
      "01/31/20 - 04:04.44PM, iter: 008, cost: 0.009289\n",
      "01/31/20 - 04:04.45PM, iter: 009, cost: 0.008974\n",
      "01/31/20 - 04:04.47PM, iter: 010, cost: 0.008727\n",
      "01/31/20 - 04:04.49PM, iter: 011, cost: 0.008527\n",
      "01/31/20 - 04:04.50PM, iter: 012, cost: 0.008361\n",
      "01/31/20 - 04:04.52PM, iter: 013, cost: 0.008222\n",
      "01/31/20 - 04:04.53PM, iter: 014, cost: 0.008103\n",
      "01/31/20 - 04:04.55PM, iter: 015, cost: 0.008001\n",
      "01/31/20 - 04:04.57PM, iter: 016, cost: 0.007908\n",
      "01/31/20 - 04:04.59PM, iter: 017, cost: 0.007828\n",
      "01/31/20 - 04:05.00PM, iter: 018, cost: 0.007757\n",
      "01/31/20 - 04:05.02PM, iter: 019, cost: 0.007692\n",
      "01/31/20 - 04:05.03PM, iter: 020, cost: 0.007633\n",
      "01/31/20 - 04:05.05PM, iter: 021, cost: 0.007578\n",
      "01/31/20 - 04:05.07PM, iter: 022, cost: 0.007530\n",
      "01/31/20 - 04:05.08PM, iter: 023, cost: 0.007485\n",
      "01/31/20 - 04:05.10PM, iter: 024, cost: 0.007442\n",
      "01/31/20 - 04:05.11PM, iter: 025, cost: 0.007404\n",
      "TRAINING MODEL\n",
      "Read 2843613 lines.\n",
      "Initializing parameters...Using random seed 1580483111\n",
      "done.\n",
      "vector size: 32\n",
      "vocab size: 22657\n",
      "x_max: 100.000000\n",
      "alpha: 0.750000\n",
      "01/31/20 - 04:05.12PM, iter: 001, cost: 0.029439\n",
      "01/31/20 - 04:05.12PM, iter: 002, cost: 0.018842\n",
      "01/31/20 - 04:05.12PM, iter: 003, cost: 0.015911\n",
      "01/31/20 - 04:05.13PM, iter: 004, cost: 0.014586\n",
      "01/31/20 - 04:05.13PM, iter: 005, cost: 0.013425\n",
      "01/31/20 - 04:05.13PM, iter: 006, cost: 0.012364\n",
      "01/31/20 - 04:05.13PM, iter: 007, cost: 0.011417\n",
      "01/31/20 - 04:05.14PM, iter: 008, cost: 0.010614\n",
      "01/31/20 - 04:05.14PM, iter: 009, cost: 0.009952\n",
      "01/31/20 - 04:05.14PM, iter: 010, cost: 0.009422\n",
      "01/31/20 - 04:05.15PM, iter: 011, cost: 0.008985\n",
      "01/31/20 - 04:05.15PM, iter: 012, cost: 0.008619\n",
      "01/31/20 - 04:05.15PM, iter: 013, cost: 0.008309\n",
      "01/31/20 - 04:05.16PM, iter: 014, cost: 0.008056\n",
      "01/31/20 - 04:05.16PM, iter: 015, cost: 0.007833\n",
      "01/31/20 - 04:05.16PM, iter: 016, cost: 0.007642\n",
      "01/31/20 - 04:05.17PM, iter: 017, cost: 0.007472\n",
      "01/31/20 - 04:05.17PM, iter: 018, cost: 0.007322\n",
      "01/31/20 - 04:05.17PM, iter: 019, cost: 0.007184\n",
      "01/31/20 - 04:05.17PM, iter: 020, cost: 0.007066\n",
      "01/31/20 - 04:05.18PM, iter: 021, cost: 0.006955\n",
      "01/31/20 - 04:05.18PM, iter: 022, cost: 0.006852\n",
      "01/31/20 - 04:05.18PM, iter: 023, cost: 0.006761\n",
      "01/31/20 - 04:05.18PM, iter: 024, cost: 0.006673\n",
      "01/31/20 - 04:05.19PM, iter: 025, cost: 0.006596\n",
      "TRAINING MODEL\n",
      "Read 16595758 lines.\n",
      "Initializing parameters...Using random seed 1580483119\n",
      "done.\n",
      "vector size: 32\n",
      "vocab size: 22657\n",
      "x_max: 100.000000\n",
      "alpha: 0.750000\n",
      "01/31/20 - 04:05.21PM, iter: 001, cost: 0.023515\n",
      "01/31/20 - 04:05.22PM, iter: 002, cost: 0.015725\n",
      "01/31/20 - 04:05.24PM, iter: 003, cost: 0.013977\n",
      "01/31/20 - 04:05.26PM, iter: 004, cost: 0.012649\n",
      "01/31/20 - 04:05.27PM, iter: 005, cost: 0.011552\n",
      "01/31/20 - 04:05.29PM, iter: 006, cost: 0.010713\n",
      "01/31/20 - 04:05.30PM, iter: 007, cost: 0.010110\n",
      "01/31/20 - 04:05.32PM, iter: 008, cost: 0.009665\n",
      "01/31/20 - 04:05.34PM, iter: 009, cost: 0.009327\n",
      "01/31/20 - 04:05.35PM, iter: 010, cost: 0.009060\n",
      "01/31/20 - 04:05.37PM, iter: 011, cost: 0.008846\n",
      "01/31/20 - 04:05.39PM, iter: 012, cost: 0.008667\n",
      "01/31/20 - 04:05.41PM, iter: 013, cost: 0.008519\n",
      "01/31/20 - 04:05.42PM, iter: 014, cost: 0.008391\n",
      "01/31/20 - 04:05.44PM, iter: 015, cost: 0.008280\n",
      "01/31/20 - 04:05.46PM, iter: 016, cost: 0.008184\n",
      "01/31/20 - 04:05.47PM, iter: 017, cost: 0.008101\n",
      "01/31/20 - 04:05.50PM, iter: 018, cost: 0.008024\n",
      "01/31/20 - 04:05.51PM, iter: 019, cost: 0.007957\n",
      "01/31/20 - 04:05.53PM, iter: 020, cost: 0.007895\n",
      "01/31/20 - 04:05.55PM, iter: 021, cost: 0.007839\n",
      "01/31/20 - 04:05.56PM, iter: 022, cost: 0.007787\n",
      "01/31/20 - 04:05.58PM, iter: 023, cost: 0.007740\n",
      "01/31/20 - 04:06.00PM, iter: 024, cost: 0.007697\n",
      "01/31/20 - 04:06.02PM, iter: 025, cost: 0.007657\n"
     ]
    }
   ],
   "source": [
    "!gloVe/build/glove -input-file data/non_aggressive_comments_cooccurrences_shuffled.bin -vocab-file data/vocab.txt -save-file data/non_aggressive_comments_vec -verbose 2 -vector-size 32 -threads 8 -alpha 0.75 -x-max 100.0 -eta 0.05 -binary 0 -model 2\n",
    "!gloVe/build/glove -input-file data/aggressive_comments_cooccurrences_shuffled.bin -vocab-file data/vocab.txt -save-file data/aggressive_comments_vec -verbose 2 -vector-size 32 -threads 8 -alpha 0.75 -x-max 100.0 -eta 0.05 -binary 0 -model 2\n",
    "!gloVe/build/glove -input-file data/all_comments_cooccurrences_shuffled.bin -vocab-file data/vocab.txt -save-file data/all_comments_vec -verbose 2 -vector-size 32 -threads 8 -alpha 0.75 -x-max 100.0 -eta 0.05 -binary 0 -model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "with open(\"data/vocab.txt\") as fin:\n",
    "    vocab,_ = zip(*map(lambda x: x.split(\" \"), fin))\n",
    "vocab = sorted(list(vocab) + [\"<unk>\"])\n",
    "\n",
    "def save_for_tf_projector(embedding, vocab, outdir, identifier):\n",
    "    out_path_data = os.path.join(outdir, \"{}_data.tsv\".format(identifier))\n",
    "    out_path_meta = os.path.join(outdir, \"{}_meta.tsv\".format(identifier))\n",
    "\n",
    "    with open(out_path_data, \"w\") as fout:\n",
    "        for row in embedding:\n",
    "            fout.write(\"{}\\n\".format(\"\\t\".join(map(str, row.tolist()))))\n",
    "    \n",
    "    with open(out_path_meta, \"w\") as fout:\n",
    "        for word in vocab:\n",
    "            fout.write(\"{}\\n\".format(word.encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "22658it [00:00, 94949.18it/s]\n22658it [00:00, 97662.94it/s]\n22658it [00:00, 92751.19it/s]\n"
    }
   ],
   "source": [
    "def index(a, x):\n",
    "    'Locate the leftmost value exactly equal to x'\n",
    "    i = bisect_left(a, x)\n",
    "    if i != len(a) and a[i] == x:\n",
    "        return i\n",
    "    raise ValueError\n",
    "\n",
    "def load_embedding(path_, vocab):\n",
    "    dim = 32\n",
    "    mat = np.empty((len(vocab), dim))\n",
    "    \n",
    "    with open(path_) as fin:\n",
    "        for row in tqdm(fin):\n",
    "            splitted = row.replace(\"\\n\", \"\").split(\" \")\n",
    "            key, vec = splitted[0], splitted[1:]\n",
    "            mat[index(vocab, key)] = vec\n",
    "            \n",
    "    return mat\n",
    "            \n",
    "all_vec = load_embedding(\"data/all_comments_vec.txt\", vocab)\n",
    "aggressive_vec = load_embedding(\"data/aggressive_comments_vec.txt\", vocab)\n",
    "non_aggressive_vec = load_embedding(\"data/non_aggressive_comments_vec.txt\", vocab)\n",
    "\n",
    "save_for_tf_projector(all_vec, vocab, \"data\", \"all_vec\")\n",
    "save_for_tf_projector(aggressive_vec, vocab, \"data\", \"agg_vec\")\n",
    "save_for_tf_projector(non_aggressive_vec, vocab, \"data\", \"noagg_vec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wh4",
   "language": "python",
   "name": "wh4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}