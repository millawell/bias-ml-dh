{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'GloVe' already exists and is not an empty directory.\n",
      "/Users/davidlassner/research/conferences_and_papers/workshops/2020 DHd/website/material/notebooks/GloVe\n",
      "mkdir -p build\n",
      "/Users/davidlassner/research/conferences_and_papers/workshops/2020 DHd/website/material/notebooks\n"
     ]
    }
   ],
   "source": [
    "!git clone git@github.com:stanfordnlp/GloVe.git\n",
    "%cd gloVe\n",
    "!make\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the aggressive wikipedia comments data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "--2020-01-31 16:01:37--  https://ndownloader.figshare.com/files/7394506\n",
      "Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 52.19.35.46, 34.248.199.177, 108.128.77.47, ...\n",
      "Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|52.19.35.46|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/7394506/aggression_annotations.tsv [following]\n",
      "--2020-01-31 16:01:37--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/7394506/aggression_annotations.tsv\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.100.179\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.100.179|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30627328 (29M) [binary/octet-stream]\n",
      "Saving to: ‘agression_annotation.tsv’\n",
      "\n",
      "agression_annotatio 100%[===================>]  29.21M  43.2MB/s    in 0.7s    \n",
      "\n",
      "2020-01-31 16:01:38 (43.2 MB/s) - ‘agression_annotation.tsv’ saved [30627328/30627328]\n",
      "\n",
      "--2020-01-31 16:01:38--  https://ndownloader.figshare.com/files/7038038\n",
      "Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 63.35.76.44, 34.248.199.177, 108.128.77.47, ...\n",
      "Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|63.35.76.44|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/7038038/aggression_annotated_comments.tsv [following]\n",
      "--2020-01-31 16:01:38--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/7038038/aggression_annotated_comments.tsv\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.100.179\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.100.179|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 58090178 (55M) [binary/octet-stream]\n",
      "Saving to: ‘aggression_annotated_comments.tsv’\n",
      "\n",
      "aggression_annotate 100%[===================>]  55.40M  48.6MB/s    in 1.1s    \n",
      "\n",
      "2020-01-31 16:01:40 (48.6 MB/s) - ‘aggression_annotated_comments.tsv’ saved [58090178/58090178]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget https://ndownloader.figshare.com/files/7394506 -O agression_annotation.tsv\n",
    "!wget https://ndownloader.figshare.com/files/7038038 -O aggression_annotated_comments.tsv\n",
    "!mv agression_annotation.tsv data/\n",
    "!mv aggression_annotated_comments.tsv data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "aggression_annotated_comments = pd.read_csv(\"data/aggression_annotated_comments.tsv\", sep=\"\\t\")\n",
    "agression_annotation = pd.read_csv(\"data/agression_annotation.tsv\", sep=\"\\t\")\n",
    "\n",
    "agression_data = pd.merge(aggression_annotated_comments, agression_annotation, on=\"rev_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8c6f73e6a74a0185862bf1f4e1509b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=115864.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "comments = []\n",
    "labels = []\n",
    "\n",
    "def regex_filter(comment):\n",
    "    return comment.replace(\"NEWLINE_TOKEN\", \"\\t\")\n",
    "\n",
    "for rev_id, rev in tqdm(agression_data.groupby(\"rev_id\")):\n",
    "    comments.append(regex_filter(rev.iloc[0].comment))\n",
    "    labels.append(rev.aggression.sum()/len(rev) >.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split    \n",
    "train_embeddings, X_other, embeddings_labels, y_other  = train_test_split(comments, labels, random_state=123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_other, y_other, random_state=456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26fa52d390e45eb89b8bcda8b107b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='storing comments', max=86898.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9b4a611cae48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"storing comments\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             ):\n\u001b[1;32m     13\u001b[0m                 \u001b[0mout_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/wh4/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/wh4/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Envs/wh4/lib/python3.7/site-packages/spacy/lang/lex_attrs.py\u001b[0m in \u001b[0;36mlower\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"data/all_comments_plain.txt\", \"w\") as all_fout:\n",
    "    with open(\"data/aggressive_comments_plain.txt\", \"w\") as agg_fout:\n",
    "        with open(\"data/non_aggressive_comments_plain.txt\", \"w\") as non_agg_fout:\n",
    "            for comment, label in tqdm(\n",
    "                zip(tokenizer.pipe(train_embeddings), embeddings_labels),\n",
    "                total=len(train_embeddings),\n",
    "                desc=\"storing comments\"\n",
    "            ):\n",
    "                out_line = \" \".join([t.text.replace(\"\\n\", \"\\t\") for t in comment])\n",
    "                all_fout.write(\"{}\\n\".format(out_line))\n",
    "                if label == 1:\n",
    "                    agg_fout.write(\"{}\\n\".format(out_line))\n",
    "                else:\n",
    "                    non_agg_fout.write(\"{}\\n\".format(out_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg_vec_data.tsv\r\n",
      "agg_vec_meta.tsv\r\n",
      "aggression_annotated_comments.tsv\r\n",
      "aggressive_comments_cooccurrences.bin\r\n",
      "aggressive_comments_cooccurrences_shuffled.bin\r\n",
      "aggressive_comments_plain.txt\r\n",
      "aggressive_comments_vec.txt\r\n",
      "agression_annotation.tsv\r\n",
      "all_comments_cooccurrences.bin\r\n",
      "all_comments_cooccurrences_shuffled.bin\r\n",
      "all_comments_plain.txt\r\n",
      "all_comments_vec.txt\r\n",
      "all_vec_data.tsv\r\n",
      "all_vec_meta.tsv\r\n",
      "noagg_vec_data.tsv\r\n",
      "noagg_vec_meta.tsv\r\n",
      "non_aggressive_comments_cooccurrences.bin\r\n",
      "non_aggressive_comments_cooccurrences_shuffled.bin\r\n",
      "non_aggressive_comments_plain.txt\r\n",
      "non_aggressive_comments_vec.txt\r\n",
      "vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common.o      cooccur.o     glove.o       shuffle.o     vocab_count.o\r\n",
      "\u001b[31mcooccur\u001b[m\u001b[m       \u001b[31mglove\u001b[m\u001b[m         \u001b[31mshuffle\u001b[m\u001b[m       \u001b[31mvocab_count\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls gloVe/build/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[11G100000 tokens.\u001b[11G200000 tokens.\u001b[11G300000 tokens.\u001b[11G400000 tokens.\u001b[11G500000 tokens.\u001b[11G600000 tokens.\u001b[11G700000 tokens.\u001b[11G800000 tokens.\u001b[11G900000 tokens.\u001b[11G1000000 tokens.\u001b[11G1100000 tokens.\u001b[11G1200000 tokens.\u001b[11G1300000 tokens.\u001b[11G1400000 tokens.\u001b[11G1500000 tokens.\u001b[11G1600000 tokens.\u001b[11G1700000 tokens.\u001b[11G1800000 tokens.\u001b[11G1900000 tokens.\u001b[11G2000000 tokens.\u001b[11G2100000 tokens.\u001b[11G2200000 tokens.\u001b[11G2300000 tokens.\u001b[11G2400000 tokens.\u001b[11G2500000 tokens.\u001b[11G2600000 tokens.\u001b[11G2700000 tokens.\u001b[11G2800000 tokens.\u001b[11G2900000 tokens.\u001b[11G3000000 tokens.\u001b[11G3100000 tokens.\u001b[11G3200000 tokens.\u001b[11G3300000 tokens.\u001b[11G3400000 tokens.\u001b[11G3500000 tokens.\u001b[11G3600000 tokens.\u001b[11G3700000 tokens.\u001b[11G3800000 tokens.\u001b[11G3900000 tokens.\u001b[11G4000000 tokens.\u001b[11G4100000 tokens.\u001b[11G4200000 tokens.\u001b[11G4300000 tokens.\u001b[11G4400000 tokens.\u001b[11G4500000 tokens.\u001b[11G4600000 tokens.\u001b[11G4700000 tokens.\u001b[11G4800000 tokens.\u001b[11G4900000 tokens.\u001b[11G5000000 tokens.\u001b[11G5100000 tokens.\u001b[11G5200000 tokens.\u001b[11G5300000 tokens.\u001b[11G5400000 tokens.\u001b[11G5500000 tokens.\u001b[11G5600000 tokens.\u001b[11G5700000 tokens.\u001b[11G5800000 tokens.\u001b[11G5900000 tokens.\u001b[11G6000000 tokens.\u001b[11G6100000 tokens.\u001b[11G6200000 tokens.\u001b[11G6300000 tokens.\u001b[11G6400000 tokens.\u001b[11G6500000 tokens.\u001b[11G6600000 tokens.\u001b[11G6700000 tokens.\u001b[11G6800000 tokens.\u001b[11G6900000 tokens.\u001b[11G7000000 tokens.\u001b[11G7100000 tokens.\u001b[11G7200000 tokens.\u001b[11G7300000 tokens.\u001b[11G7400000 tokens.\u001b[11G7500000 tokens.\u001b[0GProcessed 7504348 tokens.\n",
      "Counted 182750 unique words.\n",
      "Truncating vocabulary at min count 10.\n",
      "Using vocabulary of size 22657.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gloVe/build/vocab_count -max-vocab 25000 -min-count 10 < data/all_comments_plain.txt > data/vocab.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cooccurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 10485784\n",
      "overflow length: 28521267\n",
      "Reading vocab from file \"data/vocab.txt\"...loaded 22657 words.\n",
      "Building lookup table...table contains 51262847 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[19G900000\u001b[19G1000000\u001b[19G1100000\u001b[19G1200000\u001b[19G1300000\u001b[19G1400000\u001b[19G1500000\u001b[19G1600000\u001b[19G1700000\u001b[19G1800000\u001b[19G1900000\u001b[19G2000000\u001b[19G2100000\u001b[19G2200000\u001b[19G2300000\u001b[19G2400000\u001b[19G2500000\u001b[19G2600000\u001b[19G2700000\u001b[19G2800000\u001b[19G2900000\u001b[19G3000000\u001b[19G3100000\u001b[19G3200000\u001b[19G3300000\u001b[19G3400000\u001b[19G3500000\u001b[19G3600000\u001b[19G3700000\u001b[19G3800000\u001b[19G3900000\u001b[19G4000000\u001b[19G4100000\u001b[19G4200000\u001b[19G4300000\u001b[19G4400000\u001b[19G4500000\u001b[19G4600000\u001b[19G4700000\u001b[19G4800000\u001b[19G4900000\u001b[19G5000000\u001b[19G5100000\u001b[19G5200000\u001b[19G5300000\u001b[19G5400000\u001b[19G5500000\u001b[19G5600000\u001b[19G5700000\u001b[19G5800000\u001b[19G5900000\u001b[19G6000000\u001b[19G6100000\u001b[19G6200000\u001b[19G6300000\u001b[19G6400000\u001b[19G6500000\u001b[19G6600000\u001b[0GProcessed 6668532 tokens.\n",
      "Writing cooccurrences to disk........2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[39G2900000 lines.\u001b[39G3000000 lines.\u001b[39G3100000 lines.\u001b[39G3200000 lines.\u001b[39G3300000 lines.\u001b[39G3400000 lines.\u001b[39G3500000 lines.\u001b[39G3600000 lines.\u001b[39G3700000 lines.\u001b[39G3800000 lines.\u001b[39G3900000 lines.\u001b[39G4000000 lines.\u001b[39G4100000 lines.\u001b[39G4200000 lines.\u001b[39G4300000 lines.\u001b[39G4400000 lines.\u001b[39G4500000 lines.\u001b[39G4600000 lines.\u001b[39G4700000 lines.\u001b[39G4800000 lines.\u001b[39G4900000 lines.\u001b[39G5000000 lines.\u001b[39G5100000 lines.\u001b[39G5200000 lines.\u001b[39G5300000 lines.\u001b[39G5400000 lines.\u001b[39G5500000 lines.\u001b[39G5600000 lines.\u001b[39G5700000 lines.\u001b[39G5800000 lines.\u001b[39G5900000 lines.\u001b[39G6000000 lines.\u001b[39G6100000 lines.\u001b[39G6200000 lines.\u001b[39G6300000 lines.\u001b[39G6400000 lines.\u001b[39G6500000 lines.\u001b[39G6600000 lines.\u001b[39G6700000 lines.\u001b[39G6800000 lines.\u001b[39G6900000 lines.\u001b[39G7000000 lines.\u001b[39G7100000 lines.\u001b[39G7200000 lines.\u001b[39G7300000 lines.\u001b[39G7400000 lines.\u001b[39G7500000 lines.\u001b[39G7600000 lines.\u001b[39G7700000 lines.\u001b[39G7800000 lines.\u001b[39G7900000 lines.\u001b[39G8000000 lines.\u001b[39G8100000 lines.\u001b[39G8200000 lines.\u001b[39G8300000 lines.\u001b[39G8400000 lines.\u001b[39G8500000 lines.\u001b[39G8600000 lines.\u001b[39G8700000 lines.\u001b[39G8800000 lines.\u001b[39G8900000 lines.\u001b[39G9000000 lines.\u001b[39G9100000 lines.\u001b[39G9200000 lines.\u001b[39G9300000 lines.\u001b[39G9400000 lines.\u001b[39G9500000 lines.\u001b[39G9600000 lines.\u001b[39G9700000 lines.\u001b[39G9800000 lines.\u001b[39G9900000 lines.\u001b[39G10000000 lines.\u001b[39G10100000 lines.\u001b[39G10200000 lines.\u001b[39G10300000 lines.\u001b[39G10400000 lines.\u001b[39G10500000 lines.\u001b[39G10600000 lines.\u001b[39G10700000 lines.\u001b[39G10800000 lines.\u001b[39G10900000 lines.\u001b[39G11000000 lines.\u001b[39G11100000 lines.\u001b[39G11200000 lines.\u001b[39G11300000 lines.\u001b[39G11400000 lines.\u001b[39G11500000 lines.\u001b[39G11600000 lines.\u001b[39G11700000 lines.\u001b[39G11800000 lines.\u001b[39G11900000 lines.\u001b[39G12000000 lines.\u001b[39G12100000 lines.\u001b[39G12200000 lines.\u001b[39G12300000 lines.\u001b[39G12400000 lines.\u001b[39G12500000 lines.\u001b[39G12600000 lines.\u001b[39G12700000 lines.\u001b[39G12800000 lines.\u001b[39G12900000 lines.\u001b[39G13000000 lines.\u001b[39G13100000 lines.\u001b[39G13200000 lines.\u001b[39G13300000 lines.\u001b[39G13400000 lines.\u001b[39G13500000 lines.\u001b[39G13600000 lines.\u001b[39G13700000 lines.\u001b[39G13800000 lines.\u001b[39G13900000 lines.\u001b[39G14000000 lines.\u001b[39G14100000 lines.\u001b[39G14200000 lines.\u001b[39G14300000 lines.\u001b[39G14400000 lines.\u001b[39G14500000 lines.\u001b[39G14600000 lines.\u001b[39G14700000 lines.\u001b[39G14800000 lines.\u001b[39G14900000 lines.\u001b[39G15000000 lines.\u001b[39G15100000 lines.\u001b[39G15200000 lines.\u001b[39G15300000 lines.\u001b[39G15400000 lines.\u001b[39G15500000 lines.\u001b[39G15600000 lines.\u001b[0GMerging cooccurrence files: processed 15671104 lines.\n",
      "\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 10485784\n",
      "overflow length: 28521267\n",
      "Reading vocab from file \"data/vocab.txt\"...loaded 22657 words.\n",
      "Building lookup table...table contains 51262847 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[0GProcessed 835816 tokens.\n",
      "Writing cooccurrences to disk........2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[0GMerging cooccurrence files: processed 2843613 lines.\n",
      "\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 10485784\n",
      "overflow length: 28521267\n",
      "Reading vocab from file \"data/vocab.txt\"...loaded 22657 words.\n",
      "Building lookup table...table contains 51262847 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[19G900000\u001b[19G1000000\u001b[19G1100000\u001b[19G1200000\u001b[19G1300000\u001b[19G1400000\u001b[19G1500000\u001b[19G1600000\u001b[19G1700000\u001b[19G1800000\u001b[19G1900000\u001b[19G2000000\u001b[19G2100000\u001b[19G2200000\u001b[19G2300000\u001b[19G2400000\u001b[19G2500000\u001b[19G2600000\u001b[19G2700000\u001b[19G2800000\u001b[19G2900000\u001b[19G3000000\u001b[19G3100000\u001b[19G3200000\u001b[19G3300000\u001b[19G3400000\u001b[19G3500000\u001b[19G3600000\u001b[19G3700000\u001b[19G3800000\u001b[19G3900000\u001b[19G4000000\u001b[19G4100000\u001b[19G4200000\u001b[19G4300000\u001b[19G4400000\u001b[19G4500000\u001b[19G4600000\u001b[19G4700000\u001b[19G4800000\u001b[19G4900000\u001b[19G5000000\u001b[19G5100000\u001b[19G5200000\u001b[19G5300000\u001b[19G5400000\u001b[19G5500000\u001b[19G5600000\u001b[19G5700000\u001b[19G5800000\u001b[19G5900000\u001b[19G6000000\u001b[19G6100000\u001b[19G6200000\u001b[19G6300000\u001b[19G6400000\u001b[19G6500000\u001b[19G6600000\u001b[19G6700000\u001b[19G6800000\u001b[19G6900000\u001b[19G7000000\u001b[19G7100000\u001b[19G7200000\u001b[19G7300000\u001b[19G7400000\u001b[19G7500000\u001b[0GProcessed 7504348 tokens.\n",
      "Writing cooccurrences to disk........2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[39G2900000 lines.\u001b[39G3000000 lines.\u001b[39G3100000 lines.\u001b[39G3200000 lines.\u001b[39G3300000 lines.\u001b[39G3400000 lines.\u001b[39G3500000 lines.\u001b[39G3600000 lines.\u001b[39G3700000 lines.\u001b[39G3800000 lines.\u001b[39G3900000 lines.\u001b[39G4000000 lines.\u001b[39G4100000 lines.\u001b[39G4200000 lines.\u001b[39G4300000 lines.\u001b[39G4400000 lines.\u001b[39G4500000 lines.\u001b[39G4600000 lines.\u001b[39G4700000 lines.\u001b[39G4800000 lines.\u001b[39G4900000 lines.\u001b[39G5000000 lines.\u001b[39G5100000 lines.\u001b[39G5200000 lines.\u001b[39G5300000 lines.\u001b[39G5400000 lines.\u001b[39G5500000 lines.\u001b[39G5600000 lines.\u001b[39G5700000 lines.\u001b[39G5800000 lines.\u001b[39G5900000 lines.\u001b[39G6000000 lines.\u001b[39G6100000 lines.\u001b[39G6200000 lines.\u001b[39G6300000 lines.\u001b[39G6400000 lines.\u001b[39G6500000 lines.\u001b[39G6600000 lines.\u001b[39G6700000 lines.\u001b[39G6800000 lines.\u001b[39G6900000 lines.\u001b[39G7000000 lines.\u001b[39G7100000 lines.\u001b[39G7200000 lines.\u001b[39G7300000 lines.\u001b[39G7400000 lines.\u001b[39G7500000 lines.\u001b[39G7600000 lines.\u001b[39G7700000 lines.\u001b[39G7800000 lines.\u001b[39G7900000 lines.\u001b[39G8000000 lines.\u001b[39G8100000 lines.\u001b[39G8200000 lines.\u001b[39G8300000 lines.\u001b[39G8400000 lines.\u001b[39G8500000 lines.\u001b[39G8600000 lines.\u001b[39G8700000 lines.\u001b[39G8800000 lines.\u001b[39G8900000 lines.\u001b[39G9000000 lines.\u001b[39G9100000 lines.\u001b[39G9200000 lines.\u001b[39G9300000 lines.\u001b[39G9400000 lines.\u001b[39G9500000 lines.\u001b[39G9600000 lines.\u001b[39G9700000 lines.\u001b[39G9800000 lines.\u001b[39G9900000 lines.\u001b[39G10000000 lines.\u001b[39G10100000 lines.\u001b[39G10200000 lines.\u001b[39G10300000 lines.\u001b[39G10400000 lines.\u001b[39G10500000 lines.\u001b[39G10600000 lines.\u001b[39G10700000 lines.\u001b[39G10800000 lines.\u001b[39G10900000 lines.\u001b[39G11000000 lines.\u001b[39G11100000 lines.\u001b[39G11200000 lines.\u001b[39G11300000 lines.\u001b[39G11400000 lines.\u001b[39G11500000 lines.\u001b[39G11600000 lines.\u001b[39G11700000 lines.\u001b[39G11800000 lines.\u001b[39G11900000 lines.\u001b[39G12000000 lines.\u001b[39G12100000 lines.\u001b[39G12200000 lines.\u001b[39G12300000 lines.\u001b[39G12400000 lines.\u001b[39G12500000 lines.\u001b[39G12600000 lines.\u001b[39G12700000 lines.\u001b[39G12800000 lines.\u001b[39G12900000 lines.\u001b[39G13000000 lines.\u001b[39G13100000 lines.\u001b[39G13200000 lines.\u001b[39G13300000 lines.\u001b[39G13400000 lines.\u001b[39G13500000 lines.\u001b[39G13600000 lines.\u001b[39G13700000 lines.\u001b[39G13800000 lines.\u001b[39G13900000 lines.\u001b[39G14000000 lines.\u001b[39G14100000 lines.\u001b[39G14200000 lines.\u001b[39G14300000 lines.\u001b[39G14400000 lines.\u001b[39G14500000 lines.\u001b[39G14600000 lines.\u001b[39G14700000 lines.\u001b[39G14800000 lines.\u001b[39G14900000 lines.\u001b[39G15000000 lines.\u001b[39G15100000 lines.\u001b[39G15200000 lines.\u001b[39G15300000 lines.\u001b[39G15400000 lines.\u001b[39G15500000 lines.\u001b[39G15600000 lines.\u001b[39G15700000 lines.\u001b[39G15800000 lines.\u001b[39G15900000 lines.\u001b[39G16000000 lines.\u001b[39G16100000 lines.\u001b[39G16200000 lines.\u001b[39G16300000 lines.\u001b[39G16400000 lines.\u001b[39G16500000 lines.\u001b[0GMerging cooccurrence files: processed 16595758 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gloVe/build/cooccur -vocab-file data/vocab.txt < data/non_aggressive_comments_plain.txt > data/non_aggressive_comments_cooccurrences.bin\n",
    "!gloVe/build/cooccur -vocab-file data/vocab.txt < data/aggressive_comments_plain.txt > data/aggressive_comments_cooccurrences.bin\n",
    "!gloVe/build/cooccur -vocab-file data/vocab.txt < data/all_comments_plain.txt > data/all_comments_cooccurrences.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed 1580483054\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 15671104 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G15671104 lines.\u001b[0GMerging temp files: processed 15671104 lines.\n",
      "\n",
      "Using random seed 1580483061\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 2843613 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G2843613 lines.\u001b[0GMerging temp files: processed 2843613 lines.\n",
      "\n",
      "Using random seed 1580483062\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 16595758 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G16595758 lines.\u001b[0GMerging temp files: processed 16595758 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gloVe/build/shuffle -verbose 2 -memory 4 < data/non_aggressive_comments_cooccurrences.bin > data/non_aggressive_comments_cooccurrences_shuffled.bin\n",
    "!gloVe/build/shuffle -verbose 2 -memory 4 < data/aggressive_comments_cooccurrences.bin > data/aggressive_comments_cooccurrences_shuffled.bin\n",
    "!gloVe/build/shuffle -verbose 2 -memory 4 < data/all_comments_cooccurrences.bin > data/all_comments_cooccurrences_shuffled.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL\n",
      "Read 15671104 lines.\n",
      "Initializing parameters...Using random seed 1580483070\n",
      "done.\n",
      "vector size: 32\n",
      "vocab size: 22657\n",
      "x_max: 100.000000\n",
      "alpha: 0.750000\n",
      "01/31/20 - 04:04.33PM, iter: 001, cost: 0.022223\n",
      "01/31/20 - 04:04.34PM, iter: 002, cost: 0.014752\n",
      "01/31/20 - 04:04.36PM, iter: 003, cost: 0.013140\n",
      "01/31/20 - 04:04.37PM, iter: 004, cost: 0.011990\n",
      "01/31/20 - 04:04.39PM, iter: 005, cost: 0.011016\n",
      "01/31/20 - 04:04.41PM, iter: 006, cost: 0.010252\n",
      "01/31/20 - 04:04.42PM, iter: 007, cost: 0.009696\n",
      "01/31/20 - 04:04.44PM, iter: 008, cost: 0.009289\n",
      "01/31/20 - 04:04.45PM, iter: 009, cost: 0.008974\n",
      "01/31/20 - 04:04.47PM, iter: 010, cost: 0.008727\n",
      "01/31/20 - 04:04.49PM, iter: 011, cost: 0.008527\n",
      "01/31/20 - 04:04.50PM, iter: 012, cost: 0.008361\n",
      "01/31/20 - 04:04.52PM, iter: 013, cost: 0.008222\n",
      "01/31/20 - 04:04.53PM, iter: 014, cost: 0.008103\n",
      "01/31/20 - 04:04.55PM, iter: 015, cost: 0.008001\n",
      "01/31/20 - 04:04.57PM, iter: 016, cost: 0.007908\n",
      "01/31/20 - 04:04.59PM, iter: 017, cost: 0.007828\n",
      "01/31/20 - 04:05.00PM, iter: 018, cost: 0.007757\n",
      "01/31/20 - 04:05.02PM, iter: 019, cost: 0.007692\n",
      "01/31/20 - 04:05.03PM, iter: 020, cost: 0.007633\n",
      "01/31/20 - 04:05.05PM, iter: 021, cost: 0.007578\n",
      "01/31/20 - 04:05.07PM, iter: 022, cost: 0.007530\n",
      "01/31/20 - 04:05.08PM, iter: 023, cost: 0.007485\n",
      "01/31/20 - 04:05.10PM, iter: 024, cost: 0.007442\n",
      "01/31/20 - 04:05.11PM, iter: 025, cost: 0.007404\n",
      "TRAINING MODEL\n",
      "Read 2843613 lines.\n",
      "Initializing parameters...Using random seed 1580483111\n",
      "done.\n",
      "vector size: 32\n",
      "vocab size: 22657\n",
      "x_max: 100.000000\n",
      "alpha: 0.750000\n",
      "01/31/20 - 04:05.12PM, iter: 001, cost: 0.029439\n",
      "01/31/20 - 04:05.12PM, iter: 002, cost: 0.018842\n",
      "01/31/20 - 04:05.12PM, iter: 003, cost: 0.015911\n",
      "01/31/20 - 04:05.13PM, iter: 004, cost: 0.014586\n",
      "01/31/20 - 04:05.13PM, iter: 005, cost: 0.013425\n",
      "01/31/20 - 04:05.13PM, iter: 006, cost: 0.012364\n",
      "01/31/20 - 04:05.13PM, iter: 007, cost: 0.011417\n",
      "01/31/20 - 04:05.14PM, iter: 008, cost: 0.010614\n",
      "01/31/20 - 04:05.14PM, iter: 009, cost: 0.009952\n",
      "01/31/20 - 04:05.14PM, iter: 010, cost: 0.009422\n",
      "01/31/20 - 04:05.15PM, iter: 011, cost: 0.008985\n",
      "01/31/20 - 04:05.15PM, iter: 012, cost: 0.008619\n",
      "01/31/20 - 04:05.15PM, iter: 013, cost: 0.008309\n",
      "01/31/20 - 04:05.16PM, iter: 014, cost: 0.008056\n",
      "01/31/20 - 04:05.16PM, iter: 015, cost: 0.007833\n",
      "01/31/20 - 04:05.16PM, iter: 016, cost: 0.007642\n",
      "01/31/20 - 04:05.17PM, iter: 017, cost: 0.007472\n",
      "01/31/20 - 04:05.17PM, iter: 018, cost: 0.007322\n",
      "01/31/20 - 04:05.17PM, iter: 019, cost: 0.007184\n",
      "01/31/20 - 04:05.17PM, iter: 020, cost: 0.007066\n",
      "01/31/20 - 04:05.18PM, iter: 021, cost: 0.006955\n",
      "01/31/20 - 04:05.18PM, iter: 022, cost: 0.006852\n",
      "01/31/20 - 04:05.18PM, iter: 023, cost: 0.006761\n",
      "01/31/20 - 04:05.18PM, iter: 024, cost: 0.006673\n",
      "01/31/20 - 04:05.19PM, iter: 025, cost: 0.006596\n",
      "TRAINING MODEL\n",
      "Read 16595758 lines.\n",
      "Initializing parameters...Using random seed 1580483119\n",
      "done.\n",
      "vector size: 32\n",
      "vocab size: 22657\n",
      "x_max: 100.000000\n",
      "alpha: 0.750000\n",
      "01/31/20 - 04:05.21PM, iter: 001, cost: 0.023515\n",
      "01/31/20 - 04:05.22PM, iter: 002, cost: 0.015725\n",
      "01/31/20 - 04:05.24PM, iter: 003, cost: 0.013977\n",
      "01/31/20 - 04:05.26PM, iter: 004, cost: 0.012649\n",
      "01/31/20 - 04:05.27PM, iter: 005, cost: 0.011552\n",
      "01/31/20 - 04:05.29PM, iter: 006, cost: 0.010713\n",
      "01/31/20 - 04:05.30PM, iter: 007, cost: 0.010110\n",
      "01/31/20 - 04:05.32PM, iter: 008, cost: 0.009665\n",
      "01/31/20 - 04:05.34PM, iter: 009, cost: 0.009327\n",
      "01/31/20 - 04:05.35PM, iter: 010, cost: 0.009060\n",
      "01/31/20 - 04:05.37PM, iter: 011, cost: 0.008846\n",
      "01/31/20 - 04:05.39PM, iter: 012, cost: 0.008667\n",
      "01/31/20 - 04:05.41PM, iter: 013, cost: 0.008519\n",
      "01/31/20 - 04:05.42PM, iter: 014, cost: 0.008391\n",
      "01/31/20 - 04:05.44PM, iter: 015, cost: 0.008280\n",
      "01/31/20 - 04:05.46PM, iter: 016, cost: 0.008184\n",
      "01/31/20 - 04:05.47PM, iter: 017, cost: 0.008101\n",
      "01/31/20 - 04:05.50PM, iter: 018, cost: 0.008024\n",
      "01/31/20 - 04:05.51PM, iter: 019, cost: 0.007957\n",
      "01/31/20 - 04:05.53PM, iter: 020, cost: 0.007895\n",
      "01/31/20 - 04:05.55PM, iter: 021, cost: 0.007839\n",
      "01/31/20 - 04:05.56PM, iter: 022, cost: 0.007787\n",
      "01/31/20 - 04:05.58PM, iter: 023, cost: 0.007740\n",
      "01/31/20 - 04:06.00PM, iter: 024, cost: 0.007697\n",
      "01/31/20 - 04:06.02PM, iter: 025, cost: 0.007657\n"
     ]
    }
   ],
   "source": [
    "!gloVe/build/glove -input-file data/non_aggressive_comments_cooccurrences_shuffled.bin -vocab-file data/vocab.txt -save-file data/non_aggressive_comments_vec -verbose 2 -vector-size 32 -threads 8 -alpha 0.75 -x-max 100.0 -eta 0.05 -binary 0 -model 2\n",
    "!gloVe/build/glove -input-file data/aggressive_comments_cooccurrences_shuffled.bin -vocab-file data/vocab.txt -save-file data/aggressive_comments_vec -verbose 2 -vector-size 32 -threads 8 -alpha 0.75 -x-max 100.0 -eta 0.05 -binary 0 -model 2\n",
    "!gloVe/build/glove -input-file data/all_comments_cooccurrences_shuffled.bin -vocab-file data/vocab.txt -save-file data/all_comments_vec -verbose 2 -vector-size 32 -threads 8 -alpha 0.75 -x-max 100.0 -eta 0.05 -binary 0 -model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "with open(\"data/vocab.txt\") as fin:\n",
    "    vocab,_ = zip(*map(lambda x: x.split(\" \"), fin))\n",
    "vocab = sorted(list(vocab) + [\"<unk>\"])\n",
    "\n",
    "def save_for_tf_projector(embedding, vocab, outdir, identifier):\n",
    "    out_path_data = os.path.join(outdir, \"{}_data.tsv\".format(identifier))\n",
    "    out_path_meta = os.path.join(outdir, \"{}_meta.tsv\".format(identifier))\n",
    "\n",
    "    with open(out_path_data, \"w\") as fout:\n",
    "        for row in embedding:\n",
    "            fout.write(\"{}\\n\".format(\"\\t\".join(map(str, row.tolist()))))\n",
    "    \n",
    "    with open(out_path_meta, \"w\") as fout:\n",
    "        for word in vocab:\n",
    "            fout.write(\"{}\\n\".format(word.encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7508ac34c74fc2b91a2b0691312253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afec62412844132a71e91befa1806d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95d32dabc874163953ae1b3098081ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def index(a, x):\n",
    "    'Locate the leftmost value exactly equal to x'\n",
    "    i = bisect_left(a, x)\n",
    "    if i != len(a) and a[i] == x:\n",
    "        return i\n",
    "    raise ValueError\n",
    "\n",
    "def load_embedding(path_, vocab):\n",
    "    dim = 32\n",
    "    mat = np.empty((len(vocab), dim))\n",
    "    \n",
    "    with open(path_) as fin:\n",
    "        for row in tqdm(fin):\n",
    "            splitted = row.replace(\"\\n\", \"\").split(\" \")\n",
    "            key, vec = splitted[0], splitted[1:]\n",
    "            mat[index(vocab, key)] = vec\n",
    "            \n",
    "    return mat\n",
    "            \n",
    "all_vec = load_embedding(\"data/all_comments_vec.txt\", vocab)\n",
    "aggressive_vec = load_embedding(\"data/aggressive_comments_vec.txt\", vocab)\n",
    "non_aggressive_vec = load_embedding(\"data/non_aggressive_comments_vec.txt\", vocab)\n",
    "\n",
    "save_for_tf_projector(all_vec, vocab, \"data\", \"all_vec\")\n",
    "save_for_tf_projector(aggressive_vec, vocab, \"data\", \"agg_vec\")\n",
    "save_for_tf_projector(non_aggressive_vec, vocab, \"data\", \"noagg_vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22658"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe7cbd7bcc643839c492df90f99a933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21724.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "misses/hits 0.09561145453507156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8493a1bf8a4f54ac2b8a144a3964b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7242.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "misses/hits 0.08990938594837083\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      0.99      0.95      6292\n",
      "        True       0.85      0.37      0.51       950\n",
      "\n",
      "    accuracy                           0.91      7242\n",
      "   macro avg       0.88      0.68      0.73      7242\n",
      "weighted avg       0.90      0.91      0.89      7242\n",
      "\n",
      "aggressive_vec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6865aeed89204babacd197e7a583fc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21724.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "misses/hits 0.09561145453507156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821c4b699c3543b4be8415020caf277f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7242.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "misses/hits 0.08990938594837083\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.90      0.99      0.95      6292\n",
      "        True       0.84      0.30      0.44       950\n",
      "\n",
      "    accuracy                           0.90      7242\n",
      "   macro avg       0.87      0.64      0.69      7242\n",
      "weighted avg       0.89      0.90      0.88      7242\n",
      "\n",
      "non_aggressive_vec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0d0c9742ff4d599f4020d4cc1d2e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21724.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "misses/hits 0.09561145453507156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796a90c77a574285a74632500401ae5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7242.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "misses/hits 0.08990938594837083\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.99      0.94      6292\n",
      "        True       0.75      0.22      0.34       950\n",
      "\n",
      "    accuracy                           0.89      7242\n",
      "   macro avg       0.82      0.60      0.64      7242\n",
      "weighted avg       0.87      0.89      0.86      7242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def embed_X(X, vocab, embedding, verbose=True):\n",
    "    embedded_X = np.zeros((len(X), embedding.shape[1]))\n",
    "    num_misses = 0\n",
    "    num_hits = 0\n",
    "    for icomment, comment in tqdm(enumerate(tokenizer.pipe(X)), total=len(X)):\n",
    "        for token in comment:\n",
    "            try:\n",
    "                embedded_X[icomment] += embedding[index(vocab, token.text.replace(\"\\n\", \"\\t\"))]\n",
    "                num_hits += 1\n",
    "            except ValueError:\n",
    "                num_misses += 1\n",
    "                if verbose:\n",
    "                    print(\"{} not in vocab\".format(token))\n",
    "        embedded_X[icomment] /= len(comment)\n",
    "    \n",
    "    print(\"misses/hits {}\".format(num_misses/num_hits))\n",
    "    return embedded_X\n",
    "\n",
    "def train(X,y):\n",
    "      \n",
    "    #clf = KNeighborsClassifier()\n",
    "    clf = svm.SVC()\n",
    "    \n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "def evaluate(clf, X, y_true):\n",
    "    y_pred = clf.predict(X)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"all\")\n",
    "\n",
    "clf = train(embed_X(X_train, vocab, all_vec, verbose=False), y_train)\n",
    "evaluate(clf, embed_X(X_test, vocab, all_vec, verbose=False), y_test)\n",
    "\n",
    "print(\"aggressive_vec\")\n",
    "\n",
    "clf = train(embed_X(X_train, vocab, aggressive_vec, verbose=False), y_train)\n",
    "evaluate(clf, embed_X(X_test, vocab, aggressive_vec, verbose=False), y_test)\n",
    "\n",
    "print(\"non_aggressive_vec\")\n",
    "\n",
    "clf = train(embed_X(X_train, vocab, non_aggressive_vec, verbose=False), y_train)\n",
    "evaluate(clf, embed_X(X_test, vocab, non_aggressive_vec, verbose=False), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cfaf4f7b4b4f0b8be39717c5c4f785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21724.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "misses/hits 0.09848288826812693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89467017e57a4b9d999f30004ce55100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7242.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "misses/hits 0.09583071954045633\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "def encode_X(X, vocab, verbose=True, max_len=120):\n",
    "    encoded_X = np.zeros((len(X), max_len))\n",
    "    num_misses = 0\n",
    "    num_hits = 0\n",
    "    for icomment, comment in tqdm(enumerate(tokenizer.pipe(X)), total=len(X)):\n",
    "        for itoken, token in enumerate(comment):\n",
    "            if itoken <max_len:\n",
    "                try:\n",
    "                    encoded_X[icomment, itoken] = index(vocab, token.text.replace(\"\\n\", \"\\t\"))\n",
    "                    num_hits += 1\n",
    "                except ValueError:\n",
    "                    num_misses += 1\n",
    "                    if verbose:\n",
    "                        print(\"{} not in vocab\".format(token))\n",
    "    \n",
    "    print(\"misses/hits {}\".format(num_misses/num_hits))\n",
    "    return encoded_X\n",
    "\n",
    "encoded_X_train = torch.from_numpy(encode_X(X_train, vocab, verbose=False)).long()#[:1000]\n",
    "encoded_X_test = torch.from_numpy(encode_X(X_test, vocab, verbose=False)).long()#[:1000]\n",
    "#y_train = y_train\n",
    "#y_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfa4961bce94b77862ab03cbc486d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1357.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### EPOCH 0 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidlassner/Envs/wh4/lib/python3.7/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75afd31b8234844a89cee230bcbbfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=452.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss = 0.4009570363653699, val_loss = 0.39143057058738395, test_accuracy = 0.8689159292035398\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a46963fc0bb4fc8950b37a3cd429aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1357.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### EPOCH 1 ###\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1dd2efc48b496cbffec163d5e7cc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=452.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss = 0.3749458155894684, val_loss = 0.3811656681705365, test_accuracy = 0.8689159292035398\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c231da4da9cc484a8d40c7b412a62000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1357.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### EPOCH 2 ###\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-95fc7c80876f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/wh4/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/wh4/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def it_batches(X,y, batch_size=16):\n",
    "    \n",
    "    permutation = torch.randperm(X.size()[0])\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    for i in range(0,X.size()[0], batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        yield X[indices], y[indices]\n",
    "\n",
    "\n",
    "        \n",
    "class WordAttNet(nn.Module):\n",
    "    \"\"\"\n",
    "    @author: Viet Nguyen <nhviet1009@gmail.com>\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_matrix, hidden_size=50):\n",
    "        super(WordAttNet, self).__init__()\n",
    "        \n",
    "            \n",
    "        vocab_size, embed_size = embedding_matrix.shape\n",
    "        \n",
    "        self.word_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 2 * hidden_size))\n",
    "        self.word_bias = nn.Parameter(torch.Tensor(1, 2 * hidden_size))\n",
    "        self.context_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n",
    "        \n",
    "        self.lookup = nn.Embedding(*embedding_matrix.shape)\n",
    "        self.lookup.weight = nn.Parameter(torch.from_numpy(embedding_matrix))\n",
    "        self.lookup.weight.requires_grad = False\n",
    "    \n",
    "        self.gru = nn.GRU(embed_size, hidden_size, bidirectional=True)\n",
    "        self._create_weights(mean=0.0, std=0.05)\n",
    "        self.fc = nn.Linear(2 * hidden_size, 2)\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "\n",
    "        self.word_weight.data.normal_(mean, std)\n",
    "        self.context_weight.data.normal_(mean, std)\n",
    "\n",
    "    def forward(self, input, hidden_state):\n",
    "        output = self.lookup(input)\n",
    "        output = output.permute(1,0,2)\n",
    "\n",
    "        f_output, h_output = self.gru(output.float(), hidden_state)  # feature output and hidden state output\n",
    "        output = matrix_mul(f_output, self.word_weight, self.word_bias)\n",
    "        output = matrix_mul(output, self.context_weight).permute(1,0)\n",
    "\n",
    "        output = F.softmax(output)\n",
    "        output = element_wise_mul(f_output,output.permute(1,0))\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, h_output\n",
    "\n",
    "batch_size = 16\n",
    "hidden_size = 32\n",
    "model = WordAttNet(embedding_matrix=all_vec, hidden_size=hidden_size)\n",
    "\n",
    "def element_wise_mul(input1, input2):\n",
    "\n",
    "    feature_list = []\n",
    "    for feature_1, feature_2 in zip(input1, input2):\n",
    "        feature_2 = feature_2.unsqueeze(1).expand_as(feature_1)\n",
    "        feature = feature_1 * feature_2\n",
    "        feature_list.append(feature.unsqueeze(0))\n",
    "    output = torch.cat(feature_list, 0)\n",
    "    return torch.sum(output, 0).unsqueeze(0)\n",
    "    \n",
    "def matrix_mul(input, weight, bias=False):\n",
    "    feature_list = []\n",
    "    for feature in input:\n",
    "        feature = torch.mm(feature, weight)\n",
    "        if isinstance(bias, torch.nn.parameter.Parameter):\n",
    "            feature = feature + bias.expand(feature.size()[0], bias.size()[1])\n",
    "        feature = torch.tanh(feature).unsqueeze(0)\n",
    "        feature_list.append(feature)\n",
    "\n",
    "    return torch.cat(feature_list, 0).squeeze()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "train_loss_hist = []\n",
    "test_loss_hist = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    pbar = tqdm(it_batches(encoded_X_train, y_train, batch_size=batch_size), total=len(encoded_X_train)//batch_size)\n",
    "    \n",
    "    print(\"### EPOCH {} ###\".format(epoch))\n",
    "    model.train()\n",
    "    c_loss = 0\n",
    "    n_batches = 0\n",
    "    for X_batch, y_batch in pbar:\n",
    "        if len(X_batch) == batch_size:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            hidden_state = torch.zeros(2, batch_size, hidden_size)\n",
    "\n",
    "            output,_ = model(X_batch, hidden_state)\n",
    "            loss = criterion(output.squeeze(0), y_batch)\n",
    "            y_pred = np.argmax(output.detach(), -1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            c_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        pbar.set_description(\"train loss {0:.4f}\".format(c_loss/n_batches))\n",
    "        \n",
    "    train_loss_hist.append(c_loss/n_batches)\n",
    "    \n",
    "    model.eval()\n",
    "    c_loss = 0\n",
    "    n_batches = 0\n",
    "    accuracy = 0\n",
    "    for X_batch, y_batch in tqdm(it_batches(encoded_X_test, y_test, batch_size=batch_size), desc=\"eval\", total=len(encoded_X_test)//batch_size):\n",
    "        if len(X_batch) == batch_size:\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                hidden_state = torch.zeros(2, batch_size, hidden_size)\n",
    "                output,_ = model(X_batch, hidden_state)\n",
    "                y_pred = np.argmax(output.detach(), -1)\n",
    "                \n",
    "                accuracy += accuracy_score(y_batch.numpy().ravel(), y_pred.numpy().ravel())\n",
    "            loss = criterion(output.squeeze(0), y_batch)\n",
    "\n",
    "            c_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    test_loss_hist.append(c_loss/n_batches)\n",
    "    accuracy /= n_batches\n",
    "    \n",
    "    print(\"train_loss = {}, val_loss = {}, test_accuracy = {}\".format(\n",
    "        train_loss_hist[-1],\n",
    "        test_loss_hist[-1],\n",
    "        accuracy\n",
    "    ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8688207677437172"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred.numpy().ravel())\n",
    "len(y_batch.numpy().ravel())\n",
    "\n",
    "1 - sum(y_test)/len(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wh4",
   "language": "python",
   "name": "wh4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
